{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/03_Aprendizaje-Por-Refuerzo.ipynb) üëàüèª‚Äã **Pulsar para abrir en Colab‚Äã**\n",
    "\n",
    "# ¬øC√≥mo usar estos notebooks?\n",
    "\n",
    "Si este es el primer notebook que abres en este repositorio, te recomiendo que antes leas el [Manual de uso de estos notebooks](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/docs/manual-notebooks.md) que he creado para que te familiarices con el proyecto y las distintas rutas que puedes seguir, luego puedes volver aqu√≠ y continuar.\n",
    "\n",
    "En este notebook, vamos a profundizar en la **interpretabilidad de los modelos de IA**.\n",
    "\n",
    "Por otra parte, si a√∫n no has revisado el notebook \"[00_Empieza-aqu√≠.ipynb](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/00_Empieza-aqu√≠.ipynb)\", te sugiero que le eches un vistazo primero para conocer los conceptos b√°sicos. Pero si ya tienes una idea clara de los conceptos b√°sicos y quieres pasar a la pr√°ctica, ¬°est√°s en el lugar correcto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Interpretabilidad de los Modelos\n",
    "\n",
    "Cuando entrenamos modelos de Machine Learning, uno de los mayores desaf√≠os no es solo que el modelo funcione bien, sino entender **c√≥mo** y **por qu√©** toma sus decisiones. Este es el coraz√≥n de la interpretabilidad: poder responder a preguntas como *\"¬øpor qu√© este modelo rechaz√≥ mi pr√©stamo?\"* o *\"¬øpor qu√© dice que este tumor es maligno?\"*.\n",
    "\n",
    "## ¬øPor qu√© es importante?\n",
    "\n",
    "La interpretabilidad de los modelos no es solo un tema t√©cnico, es una cuesti√≥n de confianza, responsabilidad y toma de decisiones √©ticas. Aqu√≠ te dejo tres razones principales por las que es crucial:\n",
    "\n",
    "1. **Confianza en el modelo:** Si no entiendes c√≥mo funciona un modelo, ¬øc√≥mo puedes confiar en √©l? Esto es especialmente importante en √°reas cr√≠ticas como la salud, las finanzas y la justicia, donde las decisiones tienen un impacto directo en la vida de las personas.\n",
    "\n",
    "2. **Cumplimiento de normativas:** Cada vez m√°s regulaciones exigen que los modelos de IA sean explicables, como el [GDPR en Europa](https://gdpr-info.eu/), que establece el \"derecho a explicaci√≥n\" para decisiones automatizadas.\n",
    "\n",
    "> **Recursos:** Si quieres saber m√°s sobre el GDPR, te recomiendo explorar este [GPT personalizado](https://chatgpt.com/g/g-JvCAAqPCj-gdpr-expert), est√° muy bien construido.\n",
    "\n",
    "3. **Impacto social y √©tico:** Un modelo opaco puede amplificar sesgos y tomar decisiones injustas. Por ejemplo, si un modelo de contrataci√≥n favorece a un grupo sobre otro sin una justificaci√≥n clara, estar√≠amos perpetuando desigualdades.\n",
    "\n",
    "En resumen, la interpretabilidad no es solo un *\"nice to have\"*, es una necesidad fundamental en cualquier aplicaci√≥n de IA que impacte el mundo real.\n",
    "\n",
    "\n",
    "## ¬øQu√© vamos a ver?\n",
    "\n",
    "En este notebook, nos sumergiremos en el fascinante mundo de la interpretabilidad y exploraremos c√≥mo podemos explicar y justificar las decisiones de dos tipos de modelos muy diferentes: un **√°rbol de decisi√≥n** y una **red neuronal**.\n",
    "\n",
    "Al final de este notebook, habr√°s aprendido:\n",
    "\n",
    "- **Qu√© es la interpretabilidad y por qu√© importa**, con ejemplos del mundo real.  \n",
    "- **C√≥mo entrenar y visualizar un √°rbol de decisi√≥n,** uno de los modelos m√°s intuitivos.  \n",
    "- **C√≥mo trabajar con redes neuronales y explorar su interpretabilidad,** utilizando herramientas como SHAP.  \n",
    "- **La comparaci√≥n entre ambos enfoques,** destacando los pros y los contras de cada uno.\n",
    "\n",
    "La meta no es solo entender qu√© modelo funciona mejor, sino tambi√©n aprender a tomar decisiones informadas sobre cu√°ndo priorizar la interpretabilidad frente a la precisi√≥n.\n",
    "\n",
    "**¬øListo para desentra√±ar las decisiones de tus modelos y convertirte en un maestro de la interpretabilidad?**\n",
    "\n",
    "**¬°Vamos all√°! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √çndice de Contenidos\n",
    "\n",
    "```\n",
    "1. Fundamentos de la Interpretabilidad\n",
    "   1.1. Componentes clave de la interpretabilidad  \n",
    "       1.1.1. Comprensibilidad  \n",
    "       1.1.2. Trazabilidad  \n",
    "       1.1.3. Generalizaci√≥n y robustez  \n",
    "   1.2. Dimensiones de la interpretabilidad  \n",
    "       1.2.1. Modelos intr√≠nsecamente interpretables  \n",
    "       1.2.2. Interpretabilidad post-hoc  \n",
    "   1.3. Tradeoff: Interpretabilidad vs. Complejidad  \n",
    "2. √Årboles de decisi√≥n: Explorando un modelo interpretable (Ejercicio pr√°ctico)\n",
    "   2.1. Carga y exploraci√≥n del dataset (Breast Cancer Wisconsin)\n",
    "   2.2. Entrenamiento del √°rbol de decisi√≥n\n",
    "      2.2.1. Configuraci√≥n y ajuste de hyperpar√°metros\n",
    "      2.2.2. Visualizaci√≥n y an√°lisis del √°rbol\n",
    "   2.3. An√°lisis de la importancia de features\n",
    "      2.3.1. Identificando las reglas clave\n",
    "      2.3.2. Interpretaci√≥n de decisiones espec√≠ficas\n",
    "3. Redes neuronales: Desentra√±ando la \"caja negra\" (Ejercicio pr√°ctico)\n",
    "   3.1. Construcci√≥n y entrenamiento de una red neuronal simple\n",
    "   3.2. M√©todos para interpretar redes neuronales\n",
    "      3.2.1. Uso de SHAP para analizar la importancia de features\n",
    "      3.2.2. Visualizaci√≥n de resultados y comparaci√≥n con el √°rbol de decisi√≥n\n",
    "4. Comparativa entre modelos: √Årboles de decisi√≥n vs. Redes neuronales\n",
    "   4.1. Precisi√≥n vs. Interpretabilidad\n",
    "   4.2. Ventajas y limitaciones de cada enfoque\n",
    "5. Conclusi√≥n\n",
    "   5.1. Reflexi√≥n sobre el tradeoff entre interpretabilidad y precisi√≥n\n",
    "   5.2. El futuro de la interpretabilidad en modelos avanzados\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuraciones\n",
    "# Detectar si estamos en Colab\n",
    "in_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if in_colab:\n",
    "    # Descargar el archivo visualizations.py desde el repositorio de GitHub\n",
    "    !mkdir -p /content/utils # Creamos una carpeta utils para que coincida con la estructura del repositorio\n",
    "    !wget -O utils/visualizations.py \"https://raw.githubusercontent.com/ManuelEspejo/Machine-Learning-Bases/main/utils/visualizations.py\"\n",
    "    data_dir = '/content/data' # Ruta de los datos\n",
    "else:\n",
    "    # Agregar el directorio ra√≠z al path de Python (Para ejecutar en local)\n",
    "    notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "    project_dir = os.path.dirname(notebook_dir)\n",
    "    sys.path.append(project_dir)\n",
    "    data_dir = '../data/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librer√≠as necesarias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentos de la Interpretabilidad\n",
    "\n",
    "La interpretabilidad no es solo un tema t√©cnico; es lo que conecta a las personas con los modelos. Es como tener un traductor entre el mundo de las matem√°ticas y el mundo real. Aqu√≠ exploraremos los pilares que hacen que un modelo sea interpretable y, lo m√°s importante, entendible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Componentes clave de la interpretabilidad\n",
    "\n",
    "### 1.1.1. Comprensibilidad\n",
    "\n",
    "La facilidad con la que un humano puede entender el funcionamiento interno de un modelo. Por ejemplo, un √°rbol de decisi√≥n es claro porque sigue una l√≥gica \"si esto, entonces aquello\".\n",
    "\n",
    "### 1.1.2. Trazabilidad\n",
    "\n",
    "La capacidad de descomponer una predicci√≥n en sus partes para entender qu√© factores influyeron m√°s. Es como seguir las huellas en la escena de un cr√≠men para reconstruir lo que sucedi√≥.\n",
    "\n",
    "Por ejemplo, en un modelo m√©dico que predice si un tumor es maligno, queremos saber: ¬øFue el tama√±o del tumor? ¬øSu forma? ¬øLa edad del paciente? ¬øTodas las caracter√≠sticas?\n",
    "\n",
    "Sin trazabilidad, usamos modelos ciegamente. No podemos aceptar un diagn√≥stico m√©dico sin saber en qu√© se bas√≥. Aplicar un modelo as√≠ puede sonar extremo, pero es la realidad en muchos sistemas de IA actuales, se usan modelos que no sabemos c√≥mo funcionan mucho m√°s frecuentemente de lo que podr√≠as imaginar.\n",
    "\n",
    "### 1.1.3. Generalizaci√≥n y robustez\n",
    "\n",
    "Una buena interpretabilidad no solo implica entender un modelo, sino tambi√©n confiar en que su l√≥gica se mantiene consistente ante nuevas situaciones.\n",
    "\n",
    "El mundo no es est√°tico, por lo que no queremos un modelo que s√≥lo de buenos resultados en lo que ya ha visto, sino que sea capaz de aplicar su l√≥gica a nuevas situaciones de manera efectiva y sin errores o fallos cr√≠ticos.\n",
    "\n",
    "Un modelo robusto debe ser consistente: Si sus decisiones se vuelven err√°ticas cuando los datos cambian ligeramente, la interpretabilidad pierde su valor.\n",
    "\n",
    "En definitiva, queremos un modelo que sea capaz de salir de su \"zona de confort\" y adaptarse a lo que le venga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Dimensiones de la interpretabilidad\n",
    "\n",
    "### 1.2.1. Modelo intr√≠nsecamente interpretables\n",
    "\n",
    "Algunos modelos, como los √°rboles de decisi√≥n y las regresiones lineales, son interpretables por dise√±o. Su estructura l√≥gica y directa hace que puedas explicarlos como si estuvieras contando una historia.\n",
    "\n",
    "Te pongo un ejemplo: \"Si el paciente tiene m√°s de 50 a√±os y su presi√≥n arterial es alta, hay un 80% de probabilidades de que tenga riesgo cardiovascular.\" Como ves, dos factores que influyen directamente en la variable a predecir.\n",
    "\n",
    "El problema es que estos modelos a menudo no capturan toda la complejidad de los datos. Son buenos para hacerse una idea de la situaci√≥n, pero pueden pasar por alto cosas importantes.\n",
    "\n",
    "### 1.2.2. Interpretabilidad post-hoc\n",
    "\n",
    "Aqu√≠ entran en juego herramientas como `SHAP` o `LIME`. Estas nos sirven para inspeccionar modelos m√°s complejos y entender qu√© est√° pasando dentro. Por ejemplo, SHAP te dir√° cu√°nto influy√≥ cada feature en una predicci√≥n espec√≠fica.\n",
    "\n",
    "Esto ser√≠a como tener un panel de control para una m√°quina compleja. No entiendes todo el sistema, pero las herramientas te permiten ver qu√© est√° fallando o funcionando bien.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Tradeoff: Interpretabilidad vs. Complejidad\n",
    "\n",
    "La vida es un constante balance entre simplicidad y detalle, y lo mismo ocurre con los modelos de Machine Learning. A menudo, enfrentamos el dilema de elegir entre un modelo simple y entendible o uno complejo y m√°s potente y preciso.\n",
    "\n",
    "Y, como habr√°s imaginado, no hay una respuesta o una regla que podamos aplicar a todos los casos: depender√° de la situaci√≥n.\n",
    "\n",
    "La clave est√° en decidir qu√© es m√°s importante seg√∫n el contexto. En medicina o justicia, probablemente prefieras un modelo m√°s interpretable, incluso si pierdes algo de precisi√≥n. En cambio, en aplicaciones como recomendaciones de pel√≠culas, podr√≠as priorizar la precisi√≥n, porque no pasa nada si el modelo falla, de hecho, incluso puede ser divertido y ense√±arte cosas que no sab√≠as que te gustaban.\n",
    "\n",
    "En definitiva, la interpretabilidad no es una meta en s√≠ misma, sino un medio para llegar a decisiones m√°s informadas y conscientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. √Årboles de decisi√≥n: Explorando un modelo interpretable (Ejercicio pr√°ctico)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TallerML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
