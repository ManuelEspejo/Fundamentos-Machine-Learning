{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/03_Aprendizaje-Por-Refuerzo.ipynb) üëàüèª‚Äã **Pulsar para abrir en Colab‚Äã**\n",
    "\n",
    "# ¬øC√≥mo usar estos notebooks?\n",
    "\n",
    "Si este es el primer notebook que abres en este repositorio, te recomiendo que antes leas el [Manual de uso de estos notebooks](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/docs/manual-notebooks.md) que he creado para que te familiarices con el proyecto y las distintas rutas que puedes seguir, luego puedes volver aqu√≠ y continuar.\n",
    "\n",
    "En este notebook, vamos a profundizar en la **interpretabilidad de los modelos de IA**.\n",
    "\n",
    "Por otra parte, si a√∫n no has revisado el notebook \"[00_Empieza-aqu√≠.ipynb](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/00_Empieza-aqu√≠.ipynb)\", te sugiero que le eches un vistazo primero para conocer los conceptos b√°sicos. Pero si ya tienes una idea clara de los conceptos b√°sicos y quieres pasar a la pr√°ctica, ¬°est√°s en el lugar correcto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Interpretabilidad de los Modelos\n",
    "\n",
    "Cuando entrenamos modelos de Machine Learning, uno de los mayores desaf√≠os no es solo que el modelo funcione bien, sino entender **c√≥mo** y **por qu√©** toma sus decisiones. Este es el coraz√≥n de la interpretabilidad: poder responder a preguntas como *\"¬øpor qu√© este modelo rechaz√≥ mi pr√©stamo?\"* o *\"¬øpor qu√© dice que este tumor es maligno?\"*.\n",
    "\n",
    "## ¬øPor qu√© es importante?\n",
    "\n",
    "La interpretabilidad de los modelos no es solo un tema t√©cnico, es una cuesti√≥n de confianza, responsabilidad y toma de decisiones √©ticas. Aqu√≠ te dejo tres razones principales por las que es crucial:\n",
    "\n",
    "1. **Confianza en el modelo:** Si no entiendes c√≥mo funciona un modelo, ¬øc√≥mo puedes confiar en √©l? Esto es especialmente importante en √°reas cr√≠ticas como la salud, las finanzas y la justicia, donde las decisiones tienen un impacto directo en la vida de las personas.\n",
    "\n",
    "2. **Cumplimiento de normativas:** Cada vez m√°s regulaciones exigen que los modelos de IA sean explicables, como el [GDPR en Europa](https://gdpr-info.eu/), que establece el \"derecho a explicaci√≥n\" para decisiones automatizadas.\n",
    "\n",
    "> **Recursos:** Si quieres saber m√°s sobre el GDPR, te recomiendo explorar este [GPT personalizado](https://chatgpt.com/g/g-JvCAAqPCj-gdpr-expert), est√° muy bien construido.\n",
    "\n",
    "3. **Impacto social y √©tico:** Un modelo opaco puede amplificar sesgos y tomar decisiones injustas. Por ejemplo, si un modelo de contrataci√≥n favorece a un grupo sobre otro sin una justificaci√≥n clara, estar√≠amos perpetuando desigualdades.\n",
    "\n",
    "En resumen, la interpretabilidad no es solo un *\"nice to have\"*, es una necesidad fundamental en cualquier aplicaci√≥n de IA que impacte el mundo real.\n",
    "\n",
    "\n",
    "## ¬øQu√© vamos a ver?\n",
    "\n",
    "En este notebook, nos sumergiremos en el fascinante mundo de la interpretabilidad y exploraremos c√≥mo podemos explicar y justificar las decisiones de dos tipos de modelos muy diferentes: un **√°rbol de decisi√≥n** y una **red neuronal**.\n",
    "\n",
    "Al final de este notebook, habr√°s aprendido:\n",
    "\n",
    "- **Qu√© es la interpretabilidad y por qu√© importa**, con ejemplos del mundo real.  \n",
    "- **C√≥mo entrenar y visualizar un √°rbol de decisi√≥n,** uno de los modelos m√°s intuitivos.  \n",
    "- **C√≥mo trabajar con redes neuronales y explorar su interpretabilidad,** utilizando herramientas como SHAP.  \n",
    "- **La comparaci√≥n entre ambos enfoques,** destacando los pros y los contras de cada uno.\n",
    "\n",
    "La meta no es solo entender qu√© modelo funciona mejor, sino tambi√©n aprender a tomar decisiones informadas sobre cu√°ndo priorizar la interpretabilidad frente a la precisi√≥n.\n",
    "\n",
    "**¬øListo para desentra√±ar las decisiones de tus modelos y convertirte en un maestro de la interpretabilidad?**\n",
    "\n",
    "**¬°Vamos all√°! üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √çndice de Contenidos\n",
    "\n",
    "```\n",
    "1. Fundamentos de la Interpretabilidad\n",
    "   1.1. Componentes clave de la interpretabilidad  \n",
    "       1.1.1. Comprensibilidad  \n",
    "       1.1.2. Trazabilidad  \n",
    "       1.1.3. Generalizaci√≥n y robustez  \n",
    "   1.2. Dimensiones de la interpretabilidad  \n",
    "       1.2.1. Modelos intr√≠nsecamente interpretables  \n",
    "       1.2.2. Interpretabilidad post-hoc  \n",
    "   1.3. Tradeoff: Interpretabilidad vs. Complejidad  \n",
    "2. Teor√≠a detr√°s de los modelos interpretables  \n",
    "   2.1. √Årboles de decisi√≥n: Intuici√≥n y ventajas  \n",
    "   2.2. Redes neuronales: El reto de interpretar \"cajas negras\"  \n",
    "3. Importancia de la trazabilidad en aplicaciones cr√≠ticas  \n",
    "   3.1. Casos de uso: Medicina, Finanzas y Justicia  \n",
    "   3.2. Problemas por falta de interpretabilidad  \n",
    "4. Ejercicio pr√°ctico: Comparaci√≥n entre modelos  \n",
    "   4.1. Dataset: Breast Cancer Wisconsin  \n",
    "   4.2. Entrenamiento de un √Årbol de Decisi√≥n  \n",
    "       4.2.1. Visualizaci√≥n del √°rbol y an√°lisis de decisiones  \n",
    "   4.3. Entrenamiento de una Red Neuronal  \n",
    "       4.3.1. Interpretaci√≥n mediante SHAP  \n",
    "5. Comparativa entre √Årboles de Decisi√≥n y Redes Neuronales  \n",
    "   5.1. Precisi√≥n vs. Interpretabilidad  \n",
    "   5.2. Ventajas y limitaciones de cada enfoque  \n",
    "6. Conclusi√≥n  \n",
    "   6.1. Reflexi√≥n sobre el tradeoff entre interpretabilidad y precisi√≥n  \n",
    "   6.2. El futuro de la interpretabilidad en modelos avanzados  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Configuraciones\n",
    "# Detectar si estamos en Colab\n",
    "in_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if in_colab:\n",
    "    # Descargar el archivo visualizations.py desde el repositorio de GitHub\n",
    "    !mkdir -p /content/utils # Creamos una carpeta utils para que coincida con la estructura del repositorio\n",
    "    !wget -O utils/visualizations.py \"https://raw.githubusercontent.com/ManuelEspejo/Machine-Learning-Bases/main/utils/visualizations.py\"\n",
    "    data_dir = '/content/data' # Ruta de los datos\n",
    "else:\n",
    "    # Agregar el directorio ra√≠z al path de Python (Para ejecutar en local)\n",
    "    notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "    project_dir = os.path.dirname(notebook_dir)\n",
    "    sys.path.append(project_dir)\n",
    "    data_dir = '../data/raw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar las librer√≠as necesarias\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TallerML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
