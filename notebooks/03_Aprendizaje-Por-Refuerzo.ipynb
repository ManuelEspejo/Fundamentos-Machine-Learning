{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/03_Aprendizaje-Por-Refuerzo.ipynb) üëàüèª‚Äã **Pulsar para abrir en Colab‚Äã**\n",
    "\n",
    "# ¬øC√≥mo usar estos notebooks?\n",
    "\n",
    "Si este es el primer notebook que abres en este repositorio, te recomiendo que antes leas el [Manual de uso de estos notebooks](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/docs/manual-notebooks.md) que he creado para que te familiarices con el proyecto y las distintas rutas que puedes seguir, luego puedes volver aqu√≠ y continuar.\n",
    "\n",
    "En este notebook, vamos a profundizar en el aprendizaje por refuerzo.\n",
    "\n",
    "Por otra parte, si a√∫n no has revisado el notebook \"[00_Empieza-aqu√≠.ipynb](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/00_Empieza-aqu√≠.ipynb)\", te sugiero que le eches un vistazo primero para conocer los conceptos b√°sicos. Pero si ya tienes una idea clara de qu√© es el aprendizaje no supervisado y quieres verlo en acci√≥n, ¬°est√°s en el lugar correcto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Aprendizaje por Refuerzo\n",
    "\n",
    "El Aprendizaje por Refuerzo (Reinforcement Learning, RL) es un tipo de Machine Learning donde el modelo (llamado **agente**) aprende a trav√©s de la experiencia en un entorno, intentando maximizar una recompensa acumulada a lo largo del tiempo.\n",
    "\n",
    "## ¬øPor qu√© es importante?\n",
    "\n",
    "El RL es fundamental en la inteligencia artificial moderna porque nos permite resolver problemas en los que las decisiones se deben tomar secuencialmente y las consecuencias de una acci√≥n afectan el futuro. Algunos de los avances m√°s emocionantes en IA han sido gracias al aprendizaje por refuerzo:\n",
    "\n",
    "- **Inteligencias artificiales campeonas en juegos:** Desde el m√≠tico [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) que venci√≥ a los mejores jugadores de Go, hasta bots que dominan videojuegos como [Dota 2](https://arxiv.org/abs/1912.06680) o [StarCraft](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/).\n",
    "- **Rob√≥tica avanzada:** Robots que aprenden a caminar, volar o ensamblar piezas en f√°bricas sin un manual de instrucciones.\n",
    "- **Toma de decisiones aut√≥noma:** Algoritmos que optimizan inversiones, rutas de transporte o sistemas energ√©ticos en tiempo real.\n",
    "- **Ciencia y descubrimiento:** Sistemas que dise√±an medicamentos, exploran galaxias o incluso controlan experimentos cient√≠ficos.\n",
    "\n",
    "En esencia, el RL se utiliza cuando necesitamos que una m√°quina aprende a actuar en un entorno complejo y din√°mico, en el que no hay una soluci√≥n clara de antemano.\n",
    "\n",
    "## ¬øQu√© vamos a ver?\n",
    "\n",
    "En este notebook, vamos a explorar el aprendizaje por refuerzo desde sus fundamentos hasta su implementaci√≥n pr√°ctica. El objetivo es que puedas comprender no solo c√≥mo funciona, sino tambi√©n por qu√© es tan poderoso.\n",
    "\n",
    "Al final de este notebook, entender√°s:\n",
    "\n",
    "- **Los fundamentos del RL**, incluyendo conceptos clave como agente, entorno, recompensa y pol√≠tica.\n",
    "- **C√≥mo funciona un agente RL**, su interacci√≥n con el entorno y c√≥mo aprende para maximizar una recompensa acumulada.\n",
    "- **C√≥mo implementar un modelo RL desde cero**, aplic√°ndolo al cl√°sico problema de [CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).\n",
    "- **C√≥mo aplicar RL a diferentes disciplinas**, como negocios, videojuegos, rob√≥tica y m√°s.\n",
    "\n",
    "**¬øListo para empezar a explorar el fascinante mundo del aprendizaje por refuerzo?**\n",
    "\n",
    "**¬°Empecemos!üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √çndice de Contenidos\n",
    "\n",
    "```{table} √çndice\n",
    "1. Fundamentos del Aprendizaje por Refuerzo\n",
    "   1.1. Componentes clave del RL  \n",
    "      1.1.1. Agente, entorno y recompensa  \n",
    "      1.1.2. Pol√≠tica, funci√≥n de valor y funci√≥n Q  \n",
    "   1.2. Exploraci√≥n vs. explotaci√≥n: el equilibrio perfecto  \n",
    "2. Tabular Q-Learning: Un primer paso pr√°ctico\n",
    "   2.1. Descripci√≥n del problema: un agente en una cuadr√≠cula  \n",
    "   2.2. Configuraci√≥n del entorno  \n",
    "   2.3. Implementaci√≥n del algoritmo Tabular Q-Learning  \n",
    "      2.3.1. Inicializaci√≥n de la tabla Q  \n",
    "      2.3.2. Actualizaci√≥n de valores Q  \n",
    "   2.4. Visualizaci√≥n del aprendizaje del agente  \n",
    "   2.5. Reto pr√°ctico: Ajustando par√°metros y exploraci√≥n  \n",
    "3. OpenAI Gym y el problema de CartPole\n",
    "   3.1. Introducci√≥n a OpenAI Gym  \n",
    "   3.2. Configuraci√≥n del entorno CartPole  \n",
    "   3.3. Implementaci√≥n de Q-Learning en CartPole  \n",
    "      3.3.1. Entrenamiento del agente  \n",
    "      3.3.2. Visualizaci√≥n del progreso del agente  \n",
    "   3.4. An√°lisis de resultados y reflexiones  \n",
    "4. Casos de Uso y Reflexi√≥n Final\n",
    "   4.1. Aplicaciones reales del RL  \n",
    "      4.1.1. Negocios y marketing  \n",
    "      4.1.2. Rob√≥tica  \n",
    "      4.1.3. Videojuegos  \n",
    "   4.2. Reflexi√≥n sobre las limitaciones y futuro del RL  \n",
    "   4.3. Conclusi√≥n y pr√≥ximos pasos\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Componentes clave del RL\n",
    "\n",
    "En este apartado, vamos a desglosar los componentes clave del RL para entender bien c√≥mo podemos aplicarlos a nuestros problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Agente, entorno y recompensa\n",
    "\n",
    "En el coraz√≥n del RL, tenemos tres actores principales:\n",
    "\n",
    "#### üßë‚ÄçüöÄ Agente\n",
    "\n",
    "El agente es el protagonista de nuestra historia. Es la entidad que toma decisiones, explora el mundo y aprende de sus errores. En un videojuego, el agente ser√≠a tu personaje controlado. En un robot, el agente ser√≠a el sistema que decide c√≥mo moverse.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: ¬øQu√© acci√≥n debo tomar ahora?\n",
    "\n",
    "#### üåç Entorno\n",
    "\n",
    "El entorno es el mundo donde el agente vive y act√∫a. Define las reglas del juego y las consecuencias de cada acci√≥n. Es como el tablero de un juego de mesa: puede ser un simple tablero 2D o un mundo complejo como el universo de Minecraft.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: ¬øC√≥mo responde el mundo a mis acciones?\n",
    "\n",
    "#### üèÜ Recompensa\n",
    "\n",
    "La recompensa es el \"premio\" que el agente obtiene despu√©s de tomar una acci√≥n en el entorno. Es lo que lo motiva a actuar de cierta manera. En un videojuego, la recompensa podr√≠a ser un punto extra por recoger una moneda. En un agente de bolsa,la recompensa podr√≠a ser la ganancia neta despu√©s de una transacci√≥n.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: ¬øFue buena mi acci√≥n?\n",
    "\n",
    "Las recompensas son lo que determinan c√≥mo el agente ajusta su comportamiento con el tiempo. El agente b√°sicamente vive para maximizar sus recompensas acumuladas, es su prop√≥sito vital. Seg√∫n la situaci√≥n, podemos encontrar los siguientes escenarios de recompensa:\n",
    "\n",
    "- **Recompensa positiva**: La acci√≥n tomada fue buena. El agente recibe un premio.\n",
    "  - *Ejemplo*: Un dron recibe +1 por cada segundo que se mantiene volando hacia el objetivo.\n",
    "\n",
    "- **Recompensa negativa**: La acci√≥n tomada fue mala. El agente recibe una penalizaci√≥n.\n",
    "  - *Ejemplo*: Si el dron choca contra una pared, recibe una penalizaci√≥n de -10.\n",
    "\n",
    "- **Recompensa nula**: La acci√≥n tomada no tiene consecuencias inmediatas. No es ideal, pero tampoco perjudica al agente.\n",
    "  - *Ejemplo*: El dron empieza a volar en c√≠rculos sin progresar hacia el objetivo.\n",
    "\n",
    "#### Escenarios controlados: Evitar la destruccion del mundo\n",
    "\n",
    "Como habr√°s imaginado, no siempre podemos permitir que el agente practique en el mundo real. Imagina un agente aprendiendo a volar un avi√≥n a base de prueba y error. ¬øQu√© podr√≠a salir mal? O que un agente practicando cirug√≠as a coraz√≥n abierto con pacientes reales. No, graciasüôÇ‚Äç‚ÜîÔ∏è‚Äã.\n",
    "\n",
    "Para evitar el caos (y salvar el mundo), utilizamos simuladores de mundos controlados. Estos entornos simulan de manera segura la realidad, permitiendo que el agente practique, falle y aprende sin causar da√±os en el mundo real.\n",
    "\n",
    "¬°Ojo!, esto no s√≥lo aplica a los agentes; los humanos tambi√©n necesitamos simuladores o entornos controlados para practicar, especialmente en actividades complejas como la conducci√≥n o la cirug√≠a. La diferencia es que un agente descontrolado puede tener un potencial de destrucci√≥n mucho mayor (y no se detiene a reflexionar sobre sus errores como nosotros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Pol√≠tica, funci√≥n de valor y funci√≥n Q\n",
    "\n",
    "Ahora que tenemos claro qui√©n es qui√©n, pasemos a los conceptos que gu√≠an el aprendizaje del agente:\n",
    "\n",
    "#### üéØ Pol√≠tica\n",
    "\n",
    "La pol√≠tica es como el \"cerebro\" del agente. Define qu√© acci√≥n tomar en cada situaci√≥n. Puede ser algo tan simple como una tabla de consulta o tan complejo como una red neuronal.\n",
    "\n",
    "- **Ejemplo:** Si el dron est√° cerca de una pared, la pol√≠tica podr√≠a ser: \"Girar a la izquierda para evitarla\".\n",
    "\n",
    "#### üíé Funci√≥n de valor\n",
    "\n",
    "La funci√≥n de valor le dice al agente qu√© tan bueno es estar en un estado espec√≠fico. Es como si el agente tuviera un mapa que indica qu√© lugares son seguros y cu√°les no.\n",
    "\n",
    "- **Ejemplo:** En el caso del dron, un estado cerca de una pared podr√≠a tener un valor bajo (peligroso), mientras que un estado en el centro de la habitaci√≥n tiene un valor alto (seguro).\n",
    "\n",
    "#### üî¢ Funci√≥n Q\n",
    "\n",
    "La funci√≥n Q es un nivel m√°s avanzado: no solo eval√∫a los estados, sino las acciones dentro de esos estados. Es decir, ayuda al agente a decidir cu√°l acci√≥n espec√≠fica maximizar√° la recompensa.\n",
    "\n",
    "- **Ejemplo:** Si el dron est√° en una esquina, la funci√≥n Q le dir√≠a: \"Girando a la derecha tendr√°s una mejor recompensa que avanzando hacia adelante\".\n",
    "\n",
    "\n",
    "#### Analog√≠a: En busca del tesoro\n",
    "\n",
    "Piensa en un explorador en un bosque. La pol√≠tica es su instinto para decidir si gira a la izquierda o a la derecha. La funci√≥n de valor es el mapa que usa para saber qu√© tan lejos est√° del tesoro. La funci√≥n Q combina ambos: \"Si tomo este camino, ¬øqu√© tan r√°pido llegar√© al tesoro?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Exploraci√≥n vs. explotaci√≥n: el equilibrio perfecto\n",
    "\n",
    "En el aprendizaje por refuerzo encontramos un desaf√≠o fundamental: **¬øcu√°ndo explorar y cu√°ndo explotar?**\n",
    "\n",
    "Para que un agente aprenda de manera eficiente, necesita encontrar el equilibrio perfecto entre estas dos estrategias:\n",
    "\n",
    "### Exploraci√≥n\n",
    "\n",
    "La exploraci√≥n consiste en probar nuevas acciones para descubrir m√°s sobre el entorno, incluso si no garantizan una recompensa inmediata. Es como ser un aventurero que se adentra en territorios desconocidos, con la esperanza de encontrar algo valioso.\n",
    "\n",
    "- **Ventaja**: Puede descubrir estrategias o recompensas que no eran evidentes antes.\n",
    "- **Desventaja**: Puede tomar decisiones sub√≥ptimas en el corto plazo, lo que reduce las recompensas inmediatas.\n",
    "- **Ejemplo**: Un robot que explora una nueva habitaci√≥n podr√≠a intentar atravesar una puerta que no hab√≠a detectado antes, descubriendo un camino m√°s corto hacia su objetivo.\n",
    "\n",
    "### Explotaci√≥n\n",
    "\n",
    "La explotaci√≥n significa usar el conocimiento actual para maximizar las recompensas, eligiendo las acciones que ya sabe que funcionan. Es como ir a tu restaurante favorito y pedir ese plato que nunca te falla. ¬øPor qu√© arriesgarse, verdad?\n",
    "\n",
    "- **Ventaja**: Asegura recompensas constantes y predecibles.\n",
    "- **Desventaja**: Limita el descubrimiento de estrategias potencialmente mejores.\n",
    "- **Ejemplo**: El robot, en lugar de explorar nuevas puertas, siempre usa un camino conocido para llegar a su destino, aunque podr√≠a no ser el m√°s eficiente.\n",
    "\n",
    "### ¬øC√≥mo encontrar el equilibrio?\n",
    "\n",
    "La clave est√° en balancear ambas estrategias. Un agente que solo explora nunca aprovecha lo que ha aprendido, mientras que uno que solo explota se queda atascado en soluciones sub√≥ptimas. Encontrar este equilibrio es esencial para que el agente no solo aprenda, sino que tambi√©n logre maximizar su rendimiento.\n",
    "\n",
    "Esto no es solo un dilema de los agentes; ¬°nos pasa a los humanos todo el tiempo! ¬øDeber√≠as pedir ese plato que sabes que te encanta o arriesgarte a probar algo nuevo? ¬øIr de vacaciones al mismo lugar de siempre o aventurarte a descubrir un destino desconocido? Tanto para los agentes como para nosotros, el truco est√° en ser curiosos sin dejar de aprovechar lo que ya sabemos que funciona.\n",
    "\n",
    "Y t√∫, ¬øeres m√°s explorador o explotador?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tabular Q-Learning: Un primer paso pr√°ctico\n",
    "\n",
    "El **Q-Learning** es uno de los algoritmos m√°s b√°sicos y poderosos en el aprendizaje por refuerzo. Es un m√©todo basado en tablas que permite a un agente aprender la mejor acci√≥n para tomar en cada estado de un entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Descripci√≥n del problema: un agente en una cuadr√≠cula\n",
    "\n",
    "Para este ejemplo, imaginemos que nuestro agente es un explorador en un laberinto de cuadr√≠cula. Su objetivo es encontrar el camino m√°s r√°pido hacia la salida (o el tesoro escondido) sin chocar con los muros ni caer en trampas. Cada celda del laberinto es un estado, y las acciones posibles son moverse en 4 direcciones: arriba, abajo, izquierda o derecha.\n",
    "\n",
    "Estas son las reglas del juego:\n",
    "\n",
    "- **Entorno**: Una cuadr√≠cula 2D (5x5) con una celda de inicio, una meta y varios obst√°culos.\n",
    "- **Estados**: Cada celda de la cuadr√≠cula representa un estado diferente.\n",
    "- **Acciones**: El agente puede moverse en 4 direcciones: arriba, abajo, izquierda o derecha.\n",
    "- **Recompensas**:\n",
    "  - Llegar a la meta: +10 puntos.\n",
    "  - Chocar contra un muro u obst√°culo: -5 puntos.\n",
    "  - Movimiento normal: -1 punto (penalizaci√≥n m√≠nima para evitar movimientos innecesarios).\n",
    "\n",
    "**Visualizaci√≥n del problema:**\n",
    "\n",
    "```plaintext\n",
    "S: Inicio\n",
    "G: Meta\n",
    "X: Obst√°culo\n",
    "\n",
    "    +---+---+---+---+---+\n",
    "    | S |   |   | X |   |\n",
    "    +---+---+---+---+---+\n",
    "    |   |   | X |   |   |\n",
    "    +---+---+---+---+---+\n",
    "    |   | X |   |   | G |\n",
    "    +---+---+---+---+---+\n",
    "    |   |   |   |   |   |\n",
    "    +---+---+---+---+---+\n",
    "    |   |   |   |   |   |\n",
    "    +---+---+---+---+---+\n",
    "\n",
    "```\n",
    "\n",
    "El objetivo del agente en este caso es aprender, a trav√©s de prueba y error, c√≥mo moverse desde la celda de inicio (S) hasta la celda de meta (G) mientras minimiza las penalizaciones.\n",
    "\n",
    "Ahora veamos esto en la pr√°ctica: Configurar esta cuadr√≠cula en Python, inicializar la tabla Q, y comenzar a implementar el algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Configuraci√≥n del entorno\n",
    "\n",
    "Antes de implementar Q-Learning, necesitamos construir nuestro entorno: la cuadr√≠cula 2D donde el agente aprender√° a navegar. Esto incluye definir los estados, las acciones y las reglas del juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraci√≥n inicial\n",
    "\n",
    "Vamos a definir el entorno como una cuadr√≠cula 5x5, donde cada celda representa un estado. Utilizaremos Python para estructurar la cuadr√≠cula y asignar las recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entorno de recompensas:\n",
      "[[-1 -1 -1 -5 -1]\n",
      " [-1 -1 -5 -1 -1]\n",
      " [-1 -5 -1 -1 10]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "# Tama√±o del entorno\n",
    "grid_size = 5\n",
    "\n",
    "# Matriz de recompensas\n",
    "rewards = np.full((grid_size, grid_size), -1)\n",
    "\n",
    "# Definimos la meta y los obst√°culos\n",
    "rewards[2, 4] = 10\n",
    "rewards[0, 3] = -5 \n",
    "rewards[1, 2] = -5\n",
    "rewards[2, 1] = -5\n",
    "\n",
    "# Mostrar el entorno inicial\n",
    "print(\"Entorno de recompensas:\")\n",
    "print(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir las acciones\n",
    "\n",
    "El agente puede moverse en cuatro direcciones: **arriba, abajo, izquierda, derecha**. Vamos a asignar un √≠ndice a cada acci√≥n para que sea m√°s f√°cil codificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acciones posibles\n",
    "actions = {\n",
    "    0: \"arriba\",\n",
    "    1: \"abajo\",\n",
    "    2: \"izquierda\",\n",
    "    3: \"derecha\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitar los movimientos\n",
    "\n",
    "Para evitar que el agente se salga de los l√≠mites de la cuadr√≠cula, definiremos una funci√≥n que valide sus movimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_movement(pos, action):\n",
    "    \"\"\"\n",
    "    Valida el movimiento del agente en el entorno.\n",
    "    \"\"\"\n",
    "    x, y = pos\n",
    "    if action == 0 and x > 0:  # Arriba\n",
    "        return (x - 1, y)\n",
    "    elif action == 1 and x < grid_size - 1:  # Abajo\n",
    "        return (x + 1, y)\n",
    "    elif action == 2 and y > 0:  # Izquierda\n",
    "        return (x, y - 1)\n",
    "    elif action == 3 and y < grid_size - 1:  # Derecha\n",
    "        return (x, y + 1)\n",
    "    else:\n",
    "        return pos  # Si el movimiento no es v√°lido, permanece en el mismo lugar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar el entorno\n",
    "\n",
    "Simulemos un movimiento inicial para verificar que nuestro entorno funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El agente se movi√≥ de (0, 0) a (0, 1)\n",
      "Movimiento inv√°lido: el agente permanece en (0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Posici√≥n inicial del agente\n",
    "pos_inicial = (0, 0)\n",
    "\n",
    "# Acci√≥n de prueba: moverse hacia la derecha\n",
    "nueva_pos = validate_movement(pos_inicial, 3)\n",
    "print(f\"El agente se movi√≥ de {pos_inicial} a {nueva_pos}\")\n",
    "\n",
    "# Acci√≥n inv√°lida: intentar moverse hacia arriba desde el borde superior\n",
    "nueva_pos = validate_movement((0, 0), 0)\n",
    "print(f\"Movimiento inv√°lido: el agente permanece en {nueva_pos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Implementaci√≥n del algoritmo Tabular Q-Learning\n",
    "\n",
    "El Q-Learning es un algoritmo basado en tablas donde el agente aprende a tomar decisiones optimizadas usando la funci√≥n Q, que estima la calidad de las acciones en cada estado. Puedes imaginarlo como una hoja de c√°lculo donde el agente anota qu√© tan buena es cada acci√≥n posible en cada celda del entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Inicializaci√≥n de la tabla Q\n",
    "\n",
    "La tabla Q es una matriz donde:\n",
    "\n",
    "- Las filas representan los estados.\n",
    "- Las columnas representan las acciones.\n",
    "- Los valores en la tabla indican qu√© tan buena es una acci√≥n en un estado espec√≠fico.\n",
    "\n",
    "Primero, crearemos una tabla Q llena de ceros y definiremos algunos par√°metros clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla Q inicial:\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Inicializar la tabla Q\n",
    "q_table = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "# Par√°metros del Q-Learning\n",
    "alpha = 0.1  # Tasa de aprendizaje\n",
    "gamma = 0.9  # Factor de descuento\n",
    "epsilon = 1.0  # Probabilidad inicial de exploraci√≥n\n",
    "epsilon_decay = 0.99  # Reducci√≥n de epsilon en cada episodio\n",
    "epsilon_min = 0.1  # Valor m√≠nimo de epsilon\n",
    "\n",
    "# Mostrar la tabla Q inicial\n",
    "print(\"Tabla Q inicial:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota**: Aqu√≠ podemos ver como los valores de la tabla Q valen 0 ahora mismo, esto es porque el agente no tiene experiencia. A medida que aprende, actualizaremos estos valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Actualizaci√≥n de valores Q\n",
    "\n",
    "Para que nuestro agente mejore sus decisiones con el tiempo, es fundamental que actualice continuamente su tabla Q. Este proceso de aprendizaje se basa en una regla sencilla: cada vez que el agente toma una decisi√≥n, registra la recompensa obtenida. De este modo, construye un historial que le permite identificar las acciones que conducen a los mejores resultados.\n",
    "\n",
    "El agente utiliza una forma matem√°tica para ajustar los valores de su tabla Q. La idea es combinar:\n",
    "\n",
    "1. Lo que ya sabe sobre esa acci√≥n.\n",
    "2. Lo que acaba de aprender tras tomar laa acci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso a paso del Q-Learning\n",
    "\n",
    "1. **Elegir una acci√≥n**: El agente decide si explorar (acci√≥n aleatoria) o explotar (acci√≥n con el mejor valor Q conocido).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    \"\"\"\n",
    "    Elegir acci√≥n con epsilon-greedy\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(list(actions.keys()))  # Exploraci√≥n\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Explotaci√≥n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta funci√≥n previa, el par√°metro `epsilon` es un valor entre `0` y `1` que determina la probabilidad de que el agente explore, un valor m√°s alto, implica una mayor exploraci√≥n, un valor bajo, implica una mayor explotaci√≥n.\n",
    "\n",
    "2. **Actualizar la tabla Q**: El agente observa la recompensa y el nuevo estado, y actualiza el valor Q de la acci√≥n tomada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(state, action, reward, new_state):\n",
    "    \"\"\"\n",
    "    Actualizar la tabla Q usando la f√≥rmula de Q-Learning\n",
    "    \"\"\"\n",
    "    max_q_new = np.max(q_table[new_state])\n",
    "    q_table[state][action] += alpha * (reward + gamma * max_q_new - q_table[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Simulaci√≥n de episodios**: Un episodio es un intento completo del agente para alcanzar la meta desde el inicio, los episodios se repiten hasta que el agente llega a la meta o fracasa.\n",
    "\n",
    "Al final de cada episodio, reducimos `ùúñ` (epsilon) para que el agente explore menos y explote m√°s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50/500: epsilon=0.61\n",
      "Episode 100/500: epsilon=0.37\n",
      "Episode 150/500: epsilon=0.22\n",
      "Episode 200/500: epsilon=0.13\n",
      "Episode 250/500: epsilon=0.10\n",
      "Episode 300/500: epsilon=0.10\n",
      "Episode 350/500: epsilon=0.10\n",
      "Episode 400/500: epsilon=0.10\n",
      "Episode 450/500: epsilon=0.10\n",
      "Episode 500/500: epsilon=0.10\n"
     ]
    }
   ],
   "source": [
    "# Simulaci√≥n de episodios\n",
    "episodes = 500\n",
    "for episode in range(episodes):\n",
    "    state = (0, 0)  # Inicio\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        # Elegir acci√≥n\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        # Realizar acci√≥n y observar resultado\n",
    "        new_state = validate_movement(state, action)\n",
    "        reward = rewards[new_state]\n",
    "\n",
    "        # Actualizar Q\n",
    "        update_q(state, action, reward, new_state)\n",
    "\n",
    "        # Actualizar estado\n",
    "        state = new_state\n",
    "\n",
    "        # Verificar si llegamos a la meta\n",
    "        if state == (2, 4):  # Meta\n",
    "            terminated = True\n",
    "\n",
    "    # Reducir epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Mostrar progreso\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode {episode + 1}/{episodes}: epsilon={epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TallerML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
