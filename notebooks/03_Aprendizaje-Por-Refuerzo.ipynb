{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/03_Aprendizaje-Por-Refuerzo.ipynb) üëàüèª‚Äã **Pulsar para abrir en Colab‚Äã**\n",
    "\n",
    "# ¬øC√≥mo usar estos notebooks?\n",
    "\n",
    "Si este es el primer notebook que abres en este repositorio, te recomiendo que antes leas el [Manual de uso de estos notebooks](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/docs/manual-notebooks.md) que he creado para que te familiarices con el proyecto y las distintas rutas que puedes seguir, luego puedes volver aqu√≠ y continuar.\n",
    "\n",
    "En este notebook, vamos a profundizar en el aprendizaje por refuerzo.\n",
    "\n",
    "Por otra parte, si a√∫n no has revisado el notebook \"[00_Empieza-aqu√≠.ipynb](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/00_Empieza-aqu√≠.ipynb)\", te sugiero que le eches un vistazo primero para conocer los conceptos b√°sicos. Pero si ya tienes una idea clara de qu√© es el aprendizaje no supervisado y quieres verlo en acci√≥n, ¬°est√°s en el lugar correcto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Aprendizaje por Refuerzo\n",
    "\n",
    "El Aprendizaje por Refuerzo (Reinforcement Learning, RL) es un tipo de Machine Learning donde el modelo (llamado **agente**) aprende a trav√©s de la experiencia en un entorno, intentando maximizar una recompensa acumulada a lo largo del tiempo.\n",
    "\n",
    "## ¬øPor qu√© es importante?\n",
    "\n",
    "El RL es fundamental en la inteligencia artificial moderna porque nos permite resolver problemas en los que las decisiones se deben tomar secuencialmente y las consecuencias de una acci√≥n afectan el futuro. Algunos de los avances m√°s emocionantes en IA han sido gracias al aprendizaje por refuerzo:\n",
    "\n",
    "- **Inteligencias artificiales campeonas en juegos:** Desde el m√≠tico [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) que venci√≥ a los mejores jugadores de Go, hasta bots que dominan videojuegos como [Dota 2](https://arxiv.org/abs/1912.06680) o [StarCraft](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/).\n",
    "- **Rob√≥tica avanzada:** Robots que aprenden a caminar, volar o ensamblar piezas en f√°bricas sin un manual de instrucciones.\n",
    "- **Toma de decisiones aut√≥noma:** Algoritmos que optimizan inversiones, rutas de transporte o sistemas energ√©ticos en tiempo real.\n",
    "- **Ciencia y descubrimiento:** Sistemas que dise√±an medicamentos, exploran galaxias o incluso controlan experimentos cient√≠ficos.\n",
    "\n",
    "En esencia, el RL se utiliza cuando necesitamos que una m√°quina aprende a actuar en un entorno complejo y din√°mico, en el que no hay una soluci√≥n clara de antemano.\n",
    "\n",
    "## ¬øQu√© vamos a ver?\n",
    "\n",
    "En este notebook, vamos a explorar el aprendizaje por refuerzo desde sus fundamentos hasta su implementaci√≥n pr√°ctica. El objetivo es que puedas comprender no solo c√≥mo funciona, sino tambi√©n por qu√© es tan poderoso.\n",
    "\n",
    "Al final de este notebook, entender√°s:\n",
    "\n",
    "- **Los fundamentos del RL**, incluyendo conceptos clave como agente, entorno, recompensa y pol√≠tica.\n",
    "- **C√≥mo funciona un agente RL**, su interacci√≥n con el entorno y c√≥mo aprende para maximizar una recompensa acumulada.\n",
    "- **C√≥mo implementar un modelo RL desde cero**, aplic√°ndolo al cl√°sico problema de [CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).\n",
    "- **C√≥mo aplicar RL a diferentes disciplinas**, como negocios, videojuegos, rob√≥tica y m√°s.\n",
    "\n",
    "**¬øListo para empezar a explorar el fascinante mundo del aprendizaje por refuerzo?**\n",
    "\n",
    "**¬°Empecemos!üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √çndice de Contenidos\n",
    "\n",
    "```{table} √çndice\n",
    "1. Fundamentos del Aprendizaje por Refuerzo\n",
    "   1.1. Componentes clave del RL  \n",
    "      1.1.1. Agente, entorno y recompensa  \n",
    "      1.1.2. Pol√≠tica, funci√≥n de valor y funci√≥n Q  \n",
    "   1.2. Exploraci√≥n vs. explotaci√≥n: el equilibrio perfecto  \n",
    "2. Tabular Q-Learning: Un primer paso pr√°ctico\n",
    "   2.1. Descripci√≥n del problema: un agente en una cuadr√≠cula  \n",
    "   2.2. Configuraci√≥n del entorno  \n",
    "   2.3. Implementaci√≥n del algoritmo Tabular Q-Learning  \n",
    "      2.3.1. Inicializaci√≥n de la tabla Q  \n",
    "      2.3.2. Actualizaci√≥n de valores Q  \n",
    "   2.4. Visualizaci√≥n del aprendizaje del agente  \n",
    "   2.5. Reto pr√°ctico: Ajustando par√°metros y exploraci√≥n  \n",
    "3. OpenAI Gym y el problema de CartPole\n",
    "   3.1. Introducci√≥n a OpenAI Gym  \n",
    "   3.2. Configuraci√≥n del entorno CartPole  \n",
    "   3.3. Implementaci√≥n de Q-Learning en CartPole  \n",
    "      3.3.1. Entrenamiento del agente  \n",
    "      3.3.2. Visualizaci√≥n del progreso del agente  \n",
    "   3.4. An√°lisis de resultados y reflexiones  \n",
    "4. Casos de Uso y Reflexi√≥n Final\n",
    "   4.1. Aplicaciones reales del RL  \n",
    "      4.1.1. Negocios y marketing  \n",
    "      4.1.2. Rob√≥tica  \n",
    "      4.1.3. Videojuegos  \n",
    "   4.2. Reflexi√≥n sobre las limitaciones y futuro del RL  \n",
    "   4.3. Conclusi√≥n y pr√≥ximos pasos\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Componentes clave del RL\n",
    "\n",
    "En este apartado, vamos a desglosar los componentes clave del RL para entender bien c√≥mo podemos aplicarlos a nuestros problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Agente, entorno y recompensa\n",
    "\n",
    "En el coraz√≥n del RL, tenemos tres actores principales:\n",
    "\n",
    "#### üßë‚ÄçüöÄ Agente\n",
    "\n",
    "El agente es el protagonista de nuestra historia. Es la entidad que toma decisiones, explora el mundo y aprende de sus errores. En un videojuego, el agente ser√≠a tu personaje controlado. En un robot, el agente ser√≠a el sistema que decide c√≥mo moverse.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: ¬øQu√© acci√≥n debo tomar ahora?\n",
    "\n",
    "#### üåç Entorno\n",
    "\n",
    "El entorno es el mundo donde el agente vive y act√∫a. Define las reglas del juego y las consecuencias de cada acci√≥n. Es como el tablero de un juego de mesa: puede ser un simple tablero 2D o un mundo complejo como el universo de Minecraft.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: ¬øC√≥mo responde el mundo a mis acciones?\n",
    "\n",
    "#### üèÜ Recompensa\n",
    "\n",
    "La recompensa es el \"premio\" que el agente obtiene despu√©s de tomar una acci√≥n en el entorno. Es lo que lo motiva a actuar de cierta manera. En un videojuego, la recompensa podr√≠a ser un punto extra por recoger una moneda. En un agente de bolsa,la recompensa podr√≠a ser la ganancia neta despu√©s de una transacci√≥n.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: ¬øFue buena mi acci√≥n?\n",
    "\n",
    "Las recompensas son lo que determinan c√≥mo el agente ajusta su comportamiento con el tiempo. El agente b√°sicamente vive para maximizar sus recompensas acumuladas, es su prop√≥sito vital. Seg√∫n la situaci√≥n, podemos encontrar los siguientes escenarios de recompensa:\n",
    "\n",
    "- **Recompensa positiva**: La acci√≥n tomada fue buena. El agente recibe un premio.\n",
    "  - *Ejemplo*: Un dron recibe +1 por cada segundo que se mantiene volando hacia el objetivo.\n",
    "\n",
    "- **Recompensa negativa**: La acci√≥n tomada fue mala. El agente recibe una penalizaci√≥n.\n",
    "  - *Ejemplo*: Si el dron choca contra una pared, recibe una penalizaci√≥n de -10.\n",
    "\n",
    "- **Recompensa nula**: La acci√≥n tomada no tiene consecuencias inmediatas. No es ideal, pero tampoco perjudica al agente.\n",
    "  - *Ejemplo*: El dron empieza a volar en c√≠rculos sin progresar hacia el objetivo.\n",
    "\n",
    "#### Escenarios controlados: Evitar la destruccion del mundo\n",
    "\n",
    "Como habr√°s imaginado, no siempre podemos permitir que el agente practique en el mundo real. Imagina un agente aprendiendo a volar un avi√≥n a base de prueba y error. ¬øQu√© podr√≠a salir mal? O que un agente practicando cirug√≠as a coraz√≥n abierto con pacientes reales. No, graciasüôÇ‚Äç‚ÜîÔ∏è‚Äã.\n",
    "\n",
    "Para evitar el caos (y salvar el mundo), utilizamos simuladores de mundos controlados. Estos entornos simulan de manera segura la realidad, permitiendo que el agente practique, falle y aprende sin causar da√±os en el mundo real.\n",
    "\n",
    "¬°Ojo!, esto no s√≥lo aplica a los agentes; los humanos tambi√©n necesitamos simuladores o entornos controlados para practicar, especialmente en actividades complejas como la conducci√≥n o la cirug√≠a. La diferencia es que un agente descontrolado puede tener un potencial de destrucci√≥n mucho mayor (y no se detiene a reflexionar sobre sus errores como nosotros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Pol√≠tica, funci√≥n de valor y funci√≥n Q\n",
    "\n",
    "Ahora que tenemos claro qui√©n es qui√©n, pasemos a los conceptos que gu√≠an el aprendizaje del agente:\n",
    "\n",
    "#### üéØ Pol√≠tica\n",
    "\n",
    "La pol√≠tica es como el \"cerebro\" del agente. Define qu√© acci√≥n tomar en cada situaci√≥n. Puede ser algo tan simple como una tabla de consulta o tan complejo como una red neuronal.\n",
    "\n",
    "- **Ejemplo:** Si el dron est√° cerca de una pared, la pol√≠tica podr√≠a ser: \"Girar a la izquierda para evitarla\".\n",
    "\n",
    "#### üíé Funci√≥n de valor\n",
    "\n",
    "La funci√≥n de valor le dice al agente qu√© tan bueno es estar en un estado espec√≠fico. Es como si el agente tuviera un mapa que indica qu√© lugares son seguros y cu√°les no.\n",
    "\n",
    "- **Ejemplo:** En el caso del dron, un estado cerca de una pared podr√≠a tener un valor bajo (peligroso), mientras que un estado en el centro de la habitaci√≥n tiene un valor alto (seguro).\n",
    "\n",
    "#### üî¢ Funci√≥n Q\n",
    "\n",
    "La funci√≥n Q es un nivel m√°s avanzado: no solo eval√∫a los estados, sino las acciones dentro de esos estados. Es decir, ayuda al agente a decidir cu√°l acci√≥n espec√≠fica maximizar√° la recompensa.\n",
    "\n",
    "- **Ejemplo:** Si el dron est√° en una esquina, la funci√≥n Q le dir√≠a: \"Girando a la derecha tendr√°s una mejor recompensa que avanzando hacia adelante\".\n",
    "\n",
    "\n",
    "#### Analog√≠a: En busca del tesoro\n",
    "\n",
    "Piensa en un explorador en un bosque. La pol√≠tica es su instinto para decidir si gira a la izquierda o a la derecha. La funci√≥n de valor es el mapa que usa para saber qu√© tan lejos est√° del tesoro. La funci√≥n Q combina ambos: \"Si tomo este camino, ¬øqu√© tan r√°pido llegar√© al tesoro?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Exploraci√≥n vs. explotaci√≥n: el equilibrio perfecto\n",
    "\n",
    "En el aprendizaje por refuerzo encontramos un desaf√≠o fundamental: **¬øcu√°ndo explorar y cu√°ndo explotar?**\n",
    "\n",
    "Para que un agente aprenda de manera eficiente, necesita encontrar el equilibrio perfecto entre estas dos estrategias:\n",
    "\n",
    "### Exploraci√≥n\n",
    "\n",
    "La exploraci√≥n consiste en probar nuevas acciones para descubrir m√°s sobre el entorno, incluso si no garantizan una recompensa inmediata. Es como ser un aventurero que se adentra en territorios desconocidos, con la esperanza de encontrar algo valioso.\n",
    "\n",
    "- **Ventaja**: Puede descubrir estrategias o recompensas que no eran evidentes antes.\n",
    "- **Desventaja**: Puede tomar decisiones sub√≥ptimas en el corto plazo, lo que reduce las recompensas inmediatas.\n",
    "- **Ejemplo**: Un robot que explora una nueva habitaci√≥n podr√≠a intentar atravesar una puerta que no hab√≠a detectado antes, descubriendo un camino m√°s corto hacia su objetivo.\n",
    "\n",
    "### Explotaci√≥n\n",
    "\n",
    "La explotaci√≥n significa usar el conocimiento actual para maximizar las recompensas, eligiendo las acciones que ya sabe que funcionan. Es como ir a tu restaurante favorito y pedir ese plato que nunca te falla. ¬øPor qu√© arriesgarse, verdad?\n",
    "\n",
    "- **Ventaja**: Asegura recompensas constantes y predecibles.\n",
    "- **Desventaja**: Limita el descubrimiento de estrategias potencialmente mejores.\n",
    "- **Ejemplo**: El robot, en lugar de explorar nuevas puertas, siempre usa un camino conocido para llegar a su destino, aunque podr√≠a no ser el m√°s eficiente.\n",
    "\n",
    "### ¬øC√≥mo encontrar el equilibrio?\n",
    "\n",
    "La clave est√° en balancear ambas estrategias. Un agente que solo explora nunca aprovecha lo que ha aprendido, mientras que uno que solo explota se queda atascado en soluciones sub√≥ptimas. Encontrar este equilibrio es esencial para que el agente no solo aprenda, sino que tambi√©n logre maximizar su rendimiento.\n",
    "\n",
    "Esto no es solo un dilema de los agentes; ¬°nos pasa a los humanos todo el tiempo! ¬øDeber√≠as pedir ese plato que sabes que te encanta o arriesgarte a probar algo nuevo? ¬øIr de vacaciones al mismo lugar de siempre o aventurarte a descubrir un destino desconocido? Tanto para los agentes como para nosotros, el truco est√° en ser curiosos sin dejar de aprovechar lo que ya sabemos que funciona.\n",
    "\n",
    "Y t√∫, ¬øeres m√°s explorador o explotador?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tabular Q-Learning: Un primer paso pr√°ctico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
