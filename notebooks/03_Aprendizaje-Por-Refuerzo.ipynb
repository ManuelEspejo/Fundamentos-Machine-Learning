{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/03_Aprendizaje-Por-Refuerzo.ipynb) ðŸ‘ˆðŸ»â€‹ **Pulsar para abrir en Colabâ€‹**\n",
    "\n",
    "# Â¿CÃ³mo usar estos notebooks?\n",
    "\n",
    "Si este es el primer notebook que abres en este repositorio, te recomiendo que antes leas el [Manual de uso de estos notebooks](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/docs/manual-notebooks.md) que he creado para que te familiarices con el proyecto y las distintas rutas que puedes seguir, luego puedes volver aquÃ­ y continuar.\n",
    "\n",
    "En este notebook, vamos a profundizar en el aprendizaje por refuerzo.\n",
    "\n",
    "Por otra parte, si aÃºn no has revisado el notebook \"[00_Empieza-aquÃ­.ipynb](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/00_Empieza-aquÃ­.ipynb)\", te sugiero que le eches un vistazo primero para conocer los conceptos bÃ¡sicos. Pero si ya tienes una idea clara de quÃ© es el aprendizaje no supervisado y quieres verlo en acciÃ³n, Â¡estÃ¡s en el lugar correcto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Aprendizaje por Refuerzo\n",
    "\n",
    "El Aprendizaje por Refuerzo (Reinforcement Learning, RL) es un tipo de Machine Learning donde el modelo (llamado **agente**) aprende a travÃ©s de la experiencia en un entorno, intentando maximizar una recompensa acumulada a lo largo del tiempo.\n",
    "\n",
    "## Â¿Por quÃ© es importante?\n",
    "\n",
    "El RL es fundamental en la inteligencia artificial moderna porque nos permite resolver problemas en los que las decisiones se deben tomar secuencialmente y las consecuencias de una acciÃ³n afectan el futuro. Algunos de los avances mÃ¡s emocionantes en IA han sido gracias al aprendizaje por refuerzo:\n",
    "\n",
    "- **Inteligencias artificiales campeonas en juegos:** Desde el mÃ­tico [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) que venciÃ³ a los mejores jugadores de Go, hasta bots que dominan videojuegos como [Dota 2](https://arxiv.org/abs/1912.06680) o [StarCraft](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/).\n",
    "- **RobÃ³tica avanzada:** Robots que aprenden a caminar, volar o ensamblar piezas en fÃ¡bricas sin un manual de instrucciones.\n",
    "- **Toma de decisiones autÃ³noma:** Algoritmos que optimizan inversiones, rutas de transporte o sistemas energÃ©ticos en tiempo real.\n",
    "- **Ciencia y descubrimiento:** Sistemas que diseÃ±an medicamentos, exploran galaxias o incluso controlan experimentos cientÃ­ficos.\n",
    "\n",
    "En esencia, el RL se utiliza cuando necesitamos que una mÃ¡quina aprende a actuar en un entorno complejo y dinÃ¡mico, en el que no hay una soluciÃ³n clara de antemano.\n",
    "\n",
    "## Â¿QuÃ© vamos a ver?\n",
    "\n",
    "En este notebook, vamos a explorar el aprendizaje por refuerzo desde sus fundamentos hasta su implementaciÃ³n prÃ¡ctica. El objetivo es que puedas comprender no solo cÃ³mo funciona, sino tambiÃ©n por quÃ© es tan poderoso.\n",
    "\n",
    "Al final de este notebook, entenderÃ¡s:\n",
    "\n",
    "- **Los fundamentos del RL**, incluyendo conceptos clave como agente, entorno, recompensa y polÃ­tica.\n",
    "- **CÃ³mo funciona un agente RL**, su interacciÃ³n con el entorno y cÃ³mo aprende para maximizar una recompensa acumulada.\n",
    "- **CÃ³mo implementar un modelo RL desde cero**, aplicÃ¡ndolo al clÃ¡sico problema de [CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).\n",
    "- **CÃ³mo aplicar RL a diferentes disciplinas**, como negocios, videojuegos, robÃ³tica y mÃ¡s.\n",
    "\n",
    "**Â¿Listo para empezar a explorar el fascinante mundo del aprendizaje por refuerzo?**\n",
    "\n",
    "**Â¡Empecemos!ðŸš€**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ãndice de Contenidos\n",
    "\n",
    "```{table} Ãndice\n",
    "1. Fundamentos del Aprendizaje por Refuerzo\n",
    "   1.1. Componentes clave del RL  \n",
    "      1.1.1. Agente, entorno y recompensa  \n",
    "      1.1.2. PolÃ­tica, funciÃ³n de valor y funciÃ³n Q  \n",
    "   1.2. ExploraciÃ³n vs. explotaciÃ³n: el equilibrio perfecto  \n",
    "2. Tabular Q-Learning: Un primer paso prÃ¡ctico\n",
    "   2.1. DescripciÃ³n del problema: un agente en una cuadrÃ­cula  \n",
    "   2.2. ConfiguraciÃ³n del entorno  \n",
    "   2.3. ImplementaciÃ³n del algoritmo Tabular Q-Learning  \n",
    "      2.3.1. InicializaciÃ³n de la tabla Q  \n",
    "      2.3.2. ActualizaciÃ³n de valores Q  \n",
    "   2.4. VisualizaciÃ³n del aprendizaje del agente  \n",
    "   2.5. Reto prÃ¡ctico: Ajustando parÃ¡metros y exploraciÃ³n  \n",
    "3. OpenAI Gym y el problema de CartPole\n",
    "   3.1. IntroducciÃ³n a OpenAI Gym  \n",
    "   3.2. ConfiguraciÃ³n del entorno CartPole  \n",
    "   3.3. ImplementaciÃ³n de Q-Learning en CartPole  \n",
    "      3.3.1. Entrenamiento del agente  \n",
    "      3.3.2. VisualizaciÃ³n del progreso del agente  \n",
    "   3.4. AnÃ¡lisis de resultados y reflexiones  \n",
    "4. Casos de Uso y ReflexiÃ³n Final\n",
    "   4.1. Aplicaciones reales del RL  \n",
    "      4.1.1. Negocios y marketing  \n",
    "      4.1.2. RobÃ³tica  \n",
    "      4.1.3. Videojuegos  \n",
    "   4.2. ReflexiÃ³n sobre las limitaciones y futuro del RL  \n",
    "   4.3. ConclusiÃ³n y prÃ³ximos pasos\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Componentes clave del RL\n",
    "\n",
    "En este apartado, vamos a desglosar los componentes clave del RL para entender bien cÃ³mo podemos aplicarlos a nuestros problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Agente, entorno y recompensa\n",
    "\n",
    "En el corazÃ³n del RL, tenemos tres actores principales:\n",
    "\n",
    "#### ðŸ§‘â€ðŸš€ Agente\n",
    "\n",
    "El agente es el protagonista de nuestra historia. Es la entidad que toma decisiones, explora el mundo y aprende de sus errores. En un videojuego, el agente serÃ­a tu personaje controlado. En un robot, el agente serÃ­a el sistema que decide cÃ³mo moverse.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: Â¿QuÃ© acciÃ³n debo tomar ahora?\n",
    "\n",
    "#### ðŸŒ Entorno\n",
    "\n",
    "El entorno es el mundo donde el agente vive y actÃºa. Define las reglas del juego y las consecuencias de cada acciÃ³n. Es como el tablero de un juego de mesa: puede ser un simple tablero 2D o un mundo complejo como el universo de Minecraft.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: Â¿CÃ³mo responde el mundo a mis acciones?\n",
    "\n",
    "#### ðŸ† Recompensa\n",
    "\n",
    "La recompensa es el \"premio\" que el agente obtiene despuÃ©s de tomar una acciÃ³n en el entorno. Es lo que lo motiva a actuar de cierta manera. En un videojuego, la recompensa podrÃ­a ser un punto extra por recoger una moneda. En un agente de bolsa,la recompensa podrÃ­a ser la ganancia neta despuÃ©s de una transacciÃ³n.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: Â¿Fue buena mi acciÃ³n?\n",
    "\n",
    "Las recompensas son lo que determinan cÃ³mo el agente ajusta su comportamiento con el tiempo. El agente bÃ¡sicamente vive para maximizar sus recompensas acumuladas, es su propÃ³sito vital. SegÃºn la situaciÃ³n, podemos encontrar los siguientes escenarios de recompensa:\n",
    "\n",
    "- **Recompensa positiva**: La acciÃ³n tomada fue buena. El agente recibe un premio.\n",
    "  - *Ejemplo*: Un dron recibe +1 por cada segundo que se mantiene volando hacia el objetivo.\n",
    "\n",
    "- **Recompensa negativa**: La acciÃ³n tomada fue mala. El agente recibe una penalizaciÃ³n.\n",
    "  - *Ejemplo*: Si el dron choca contra una pared, recibe una penalizaciÃ³n de -10.\n",
    "\n",
    "- **Recompensa nula**: La acciÃ³n tomada no tiene consecuencias inmediatas. No es ideal, pero tampoco perjudica al agente.\n",
    "  - *Ejemplo*: El dron empieza a volar en cÃ­rculos sin progresar hacia el objetivo.\n",
    "\n",
    "#### Escenarios controlados: Evitar la destruccion del mundo\n",
    "\n",
    "Como habrÃ¡s imaginado, no siempre podemos permitir que el agente practique en el mundo real. Imagina un agente aprendiendo a volar un aviÃ³n a base de prueba y error. Â¿QuÃ© podrÃ­a salir mal? O que un agente practicando cirugÃ­as a corazÃ³n abierto con pacientes reales. No, graciasðŸ™‚â€â†”ï¸â€‹.\n",
    "\n",
    "Para evitar el caos (y salvar el mundo), utilizamos simuladores de mundos controlados. Estos entornos simulan de manera segura la realidad, permitiendo que el agente practique, falle y aprende sin causar daÃ±os en el mundo real.\n",
    "\n",
    "Â¡Ojo!, esto no sÃ³lo aplica a los agentes; los humanos tambiÃ©n necesitamos simuladores o entornos controlados para practicar, especialmente en actividades complejas como la conducciÃ³n o la cirugÃ­a. La diferencia es que un agente descontrolado puede tener un potencial de destrucciÃ³n mucho mayor (y no se detiene a reflexionar sobre sus errores como nosotros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. PolÃ­tica, funciÃ³n de valor y funciÃ³n Q\n",
    "\n",
    "Ahora que tenemos claro quiÃ©n es quiÃ©n, pasemos a los conceptos que guÃ­an el aprendizaje del agente:\n",
    "\n",
    "#### ðŸŽ¯ PolÃ­tica\n",
    "\n",
    "La polÃ­tica es como el \"cerebro\" del agente. Define quÃ© acciÃ³n tomar en cada situaciÃ³n. Puede ser algo tan simple como una tabla de consulta o tan complejo como una red neuronal.\n",
    "\n",
    "- **Ejemplo:** Si el dron estÃ¡ cerca de una pared, la polÃ­tica podrÃ­a ser: \"Girar a la izquierda para evitarla\".\n",
    "\n",
    "#### ðŸ’Ž FunciÃ³n de valor\n",
    "\n",
    "La funciÃ³n de valor le dice al agente quÃ© tan bueno es estar en un estado especÃ­fico. Es como si el agente tuviera un mapa que indica quÃ© lugares son seguros y cuÃ¡les no.\n",
    "\n",
    "- **Ejemplo:** En el caso del dron, un estado cerca de una pared podrÃ­a tener un valor bajo (peligroso), mientras que un estado en el centro de la habitaciÃ³n tiene un valor alto (seguro).\n",
    "\n",
    "#### ðŸ”¢ FunciÃ³n Q\n",
    "\n",
    "La funciÃ³n Q es un nivel mÃ¡s avanzado: no solo evalÃºa los estados, sino las acciones dentro de esos estados. Es decir, ayuda al agente a decidir cuÃ¡l acciÃ³n especÃ­fica maximizarÃ¡ la recompensa.\n",
    "\n",
    "- **Ejemplo:** Si el dron estÃ¡ en una esquina, la funciÃ³n Q le dirÃ­a: \"Girando a la derecha tendrÃ¡s una mejor recompensa que avanzando hacia adelante\".\n",
    "\n",
    "\n",
    "#### AnalogÃ­a: En busca del tesoro\n",
    "\n",
    "Piensa en un explorador en un bosque. La polÃ­tica es su instinto para decidir si gira a la izquierda o a la derecha. La funciÃ³n de valor es el mapa que usa para saber quÃ© tan lejos estÃ¡ del tesoro. La funciÃ³n Q combina ambos: \"Si tomo este camino, Â¿quÃ© tan rÃ¡pido llegarÃ© al tesoro?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. ExploraciÃ³n vs. explotaciÃ³n: el equilibrio perfecto\n",
    "\n",
    "En el aprendizaje por refuerzo encontramos un desafÃ­o fundamental: **Â¿cuÃ¡ndo explorar y cuÃ¡ndo explotar?**\n",
    "\n",
    "Para que un agente aprenda de manera eficiente, necesita encontrar el equilibrio perfecto entre estas dos estrategias:\n",
    "\n",
    "### ExploraciÃ³n\n",
    "\n",
    "La exploraciÃ³n consiste en probar nuevas acciones para descubrir mÃ¡s sobre el entorno, incluso si no garantizan una recompensa inmediata. Es como ser un aventurero que se adentra en territorios desconocidos, con la esperanza de encontrar algo valioso.\n",
    "\n",
    "- **Ventaja**: Puede descubrir estrategias o recompensas que no eran evidentes antes.\n",
    "- **Desventaja**: Puede tomar decisiones subÃ³ptimas en el corto plazo, lo que reduce las recompensas inmediatas.\n",
    "- **Ejemplo**: Un robot que explora una nueva habitaciÃ³n podrÃ­a intentar atravesar una puerta que no habÃ­a detectado antes, descubriendo un camino mÃ¡s corto hacia su objetivo.\n",
    "\n",
    "### ExplotaciÃ³n\n",
    "\n",
    "La explotaciÃ³n significa usar el conocimiento actual para maximizar las recompensas, eligiendo las acciones que ya sabe que funcionan. Es como ir a tu restaurante favorito y pedir ese plato que nunca te falla. Â¿Por quÃ© arriesgarse, verdad?\n",
    "\n",
    "- **Ventaja**: Asegura recompensas constantes y predecibles.\n",
    "- **Desventaja**: Limita el descubrimiento de estrategias potencialmente mejores.\n",
    "- **Ejemplo**: El robot, en lugar de explorar nuevas puertas, siempre usa un camino conocido para llegar a su destino, aunque podrÃ­a no ser el mÃ¡s eficiente.\n",
    "\n",
    "### Â¿CÃ³mo encontrar el equilibrio?\n",
    "\n",
    "La clave estÃ¡ en balancear ambas estrategias. Un agente que solo explora nunca aprovecha lo que ha aprendido, mientras que uno que solo explota se queda atascado en soluciones subÃ³ptimas. Encontrar este equilibrio es esencial para que el agente no solo aprenda, sino que tambiÃ©n logre maximizar su rendimiento.\n",
    "\n",
    "Esto no es solo un dilema de los agentes; Â¡nos pasa a los humanos todo el tiempo! Â¿DeberÃ­as pedir ese plato que sabes que te encanta o arriesgarte a probar algo nuevo? Â¿Ir de vacaciones al mismo lugar de siempre o aventurarte a descubrir un destino desconocido? Tanto para los agentes como para nosotros, el truco estÃ¡ en ser curiosos sin dejar de aprovechar lo que ya sabemos que funciona.\n",
    "\n",
    "Y tÃº, Â¿eres mÃ¡s explorador o explotador?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tabular Q-Learning: Un primer paso prÃ¡ctico\n",
    "\n",
    "El **Q-Learning** es uno de los algoritmos mÃ¡s bÃ¡sicos y poderosos en el aprendizaje por refuerzo. Es un mÃ©todo basado en tablas que permite a un agente aprender la mejor acciÃ³n para tomar en cada estado de un entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. DescripciÃ³n del problema: un agente en una cuadrÃ­cula\n",
    "\n",
    "Para este ejemplo, imaginemos que nuestro agente es un explorador en un laberinto de cuadrÃ­cula. Su objetivo es encontrar el camino mÃ¡s rÃ¡pido hacia la salida (o el tesoro escondido) sin chocar con los muros ni caer en trampas. Cada celda del laberinto es un estado, y las acciones posibles son moverse en 4 direcciones: arriba, abajo, izquierda o derecha.\n",
    "\n",
    "Estas son las reglas del juego:\n",
    "\n",
    "- **Entorno**: Una cuadrÃ­cula 2D (5x5) con una celda de inicio, una meta y varios obstÃ¡culos.\n",
    "- **Estados**: Cada celda de la cuadrÃ­cula representa un estado diferente.\n",
    "- **Acciones**: El agente puede moverse en 4 direcciones: arriba, abajo, izquierda o derecha.\n",
    "- **Recompensas**:\n",
    "  - Llegar a la meta: +10 puntos.\n",
    "  - Chocar contra un muro u obstÃ¡culo: -5 puntos.\n",
    "  - Movimiento normal: -1 punto (penalizaciÃ³n mÃ­nima para evitar movimientos innecesarios).\n",
    "\n",
    "**VisualizaciÃ³n del problema:**\n",
    "\n",
    "```plaintext\n",
    "S: Inicio\n",
    "G: Meta\n",
    "X: ObstÃ¡culo\n",
    "\n",
    "    +---+---+---+---+---+\n",
    "    | S |   |   | X |   |\n",
    "    +---+---+---+---+---+\n",
    "    |   |   | X |   |   |\n",
    "    +---+---+---+---+---+\n",
    "    |   | X |   |   | G |\n",
    "    +---+---+---+---+---+\n",
    "    |   |   |   |   |   |\n",
    "    +---+---+---+---+---+\n",
    "    |   |   |   |   |   |\n",
    "    +---+---+---+---+---+\n",
    "\n",
    "```\n",
    "\n",
    "El objetivo del agente en este caso es aprender, a travÃ©s de prueba y error, cÃ³mo moverse desde la celda de inicio (S) hasta la celda de meta (G) mientras minimiza las penalizaciones.\n",
    "\n",
    "Ahora veamos esto en la prÃ¡ctica: Configurar esta cuadrÃ­cula en Python, inicializar la tabla Q, y comenzar a implementar el algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. ConfiguraciÃ³n del entorno\n",
    "\n",
    "Antes de implementar Q-Learning, necesitamos construir nuestro entorno: la cuadrÃ­cula 2D donde el agente aprenderÃ¡ a navegar. Esto incluye definir los estados, las acciones y las reglas del juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConfiguraciÃ³n inicial\n",
    "\n",
    "Vamos a definir el entorno como una cuadrÃ­cula 5x5, donde cada celda representa un estado. Utilizaremos Python para estructurar la cuadrÃ­cula y asignar las recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entorno de recompensas:\n",
      "[[-1 -1 -1 -5 -1]\n",
      " [-1 -1 -5 -1 -1]\n",
      " [-1 -5 -1 -1 10]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "# TamaÃ±o del entorno\n",
    "grid_size = 5\n",
    "\n",
    "# Matriz de recompensas\n",
    "rewards = np.full((grid_size, grid_size), -1)\n",
    "\n",
    "# Definimos la meta y los obstÃ¡culos\n",
    "rewards[2, 4] = 10\n",
    "rewards[0, 3] = -5 \n",
    "rewards[1, 2] = -5\n",
    "rewards[2, 1] = -5\n",
    "\n",
    "# Mostrar el entorno inicial\n",
    "print(\"Entorno de recompensas:\")\n",
    "print(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir las acciones\n",
    "\n",
    "El agente puede moverse en cuatro direcciones: **arriba, abajo, izquierda, derecha**. Vamos a asignar un Ã­ndice a cada acciÃ³n para que sea mÃ¡s fÃ¡cil codificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acciones posibles\n",
    "actions = {\n",
    "    0: \"arriba\",\n",
    "    1: \"abajo\",\n",
    "    2: \"izquierda\",\n",
    "    3: \"derecha\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitar los movimientos\n",
    "\n",
    "Para evitar que el agente se salga de los lÃ­mites de la cuadrÃ­cula, definiremos una funciÃ³n que valide sus movimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validar_movimiento(pos, action):\n",
    "    \"\"\"\n",
    "    Valida el movimiento del agente en el entorno.\n",
    "    \"\"\"\n",
    "    x, y = pos\n",
    "    if action == 0 and x > 0:  # Arriba\n",
    "        return (x - 1, y)\n",
    "    elif action == 1 and x < grid_size - 1:  # Abajo\n",
    "        return (x + 1, y)\n",
    "    elif action == 2 and y > 0:  # Izquierda\n",
    "        return (x, y - 1)\n",
    "    elif action == 3 and y < grid_size - 1:  # Derecha\n",
    "        return (x, y + 1)\n",
    "    else:\n",
    "        return pos  # Si el movimiento no es vÃ¡lido, permanece en el mismo lugar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar el entorno\n",
    "\n",
    "Simulemos un movimiento inicial para verificar que nuestro entorno funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El agente se moviÃ³ de (0, 0) a (0, 1)\n",
      "Movimiento invÃ¡lido: el agente permanece en (0, 0)\n"
     ]
    }
   ],
   "source": [
    "# PosiciÃ³n inicial del agente\n",
    "pos_inicial = (0, 0)\n",
    "\n",
    "# AcciÃ³n de prueba: moverse hacia la derecha\n",
    "nueva_pos = validar_movimiento(pos_inicial, 3)\n",
    "print(f\"El agente se moviÃ³ de {pos_inicial} a {nueva_pos}\")\n",
    "\n",
    "# AcciÃ³n invÃ¡lida: intentar moverse hacia arriba desde el borde superior\n",
    "nueva_pos = validar_movimiento((0, 0), 0)\n",
    "print(f\"Movimiento invÃ¡lido: el agente permanece en {nueva_pos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TallerML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
