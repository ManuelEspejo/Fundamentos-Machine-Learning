{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/03_Aprendizaje-Por-Refuerzo.ipynb) üëàüèª‚Äã **Pulsar para abrir en Colab‚Äã**\n",
    "\n",
    "# ¬øC√≥mo usar estos notebooks?\n",
    "\n",
    "Si este es el primer notebook que abres en este repositorio, te recomiendo que antes leas el [Manual de uso de estos notebooks](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/docs/manual-notebooks.md) que he creado para que te familiarices con el proyecto y las distintas rutas que puedes seguir, luego puedes volver aqu√≠ y continuar.\n",
    "\n",
    "En este notebook, vamos a profundizar en el aprendizaje por refuerzo.\n",
    "\n",
    "Por otra parte, si a√∫n no has revisado el notebook \"[00_Empieza-aqu√≠.ipynb](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/00_Empieza-aqu√≠.ipynb)\", te sugiero que le eches un vistazo primero para conocer los conceptos b√°sicos. Pero si ya tienes una idea clara de qu√© es el aprendizaje no supervisado y quieres verlo en acci√≥n, ¬°est√°s en el lugar correcto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Aprendizaje por Refuerzo\n",
    "\n",
    "El Aprendizaje por Refuerzo (Reinforcement Learning, RL) es un tipo de Machine Learning donde el modelo (llamado **agente**) aprende a trav√©s de la experiencia en un entorno, intentando maximizar una recompensa acumulada a lo largo del tiempo.\n",
    "\n",
    "## ¬øPor qu√© es importante?\n",
    "\n",
    "El RL es fundamental en la inteligencia artificial moderna porque nos permite resolver problemas en los que las decisiones se deben tomar secuencialmente y las consecuencias de una acci√≥n afectan el futuro. Algunos de los avances m√°s emocionantes en IA han sido gracias al aprendizaje por refuerzo:\n",
    "\n",
    "- **Inteligencias artificiales campeonas en juegos:** Desde el m√≠tico [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) que venci√≥ a los mejores jugadores de Go, hasta bots que dominan videojuegos como [Dota 2](https://arxiv.org/abs/1912.06680) o [StarCraft](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/).\n",
    "- **Rob√≥tica avanzada:** Robots que aprenden a caminar, volar o ensamblar piezas en f√°bricas sin un manual de instrucciones.\n",
    "- **Toma de decisiones aut√≥noma:** Algoritmos que optimizan inversiones, rutas de transporte o sistemas energ√©ticos en tiempo real.\n",
    "- **Ciencia y descubrimiento:** Sistemas que dise√±an medicamentos, exploran galaxias o incluso controlan experimentos cient√≠ficos.\n",
    "\n",
    "En esencia, el RL se utiliza cuando necesitamos que una m√°quina aprende a actuar en un entorno complejo y din√°mico, en el que no hay una soluci√≥n clara de antemano.\n",
    "\n",
    "## ¬øQu√© vamos a ver?\n",
    "\n",
    "En este notebook, vamos a explorar el aprendizaje por refuerzo desde sus fundamentos hasta su implementaci√≥n pr√°ctica. El objetivo es que puedas comprender no solo c√≥mo funciona, sino tambi√©n por qu√© es tan poderoso.\n",
    "\n",
    "Al final de este notebook, entender√°s:\n",
    "\n",
    "- **Los fundamentos del RL**, incluyendo conceptos clave como agente, entorno, recompensa y pol√≠tica.\n",
    "- **C√≥mo funciona un agente RL**, su interacci√≥n con el entorno y c√≥mo aprende para maximizar una recompensa acumulada.\n",
    "- **C√≥mo implementar un modelo RL desde cero**, aplic√°ndolo al cl√°sico problema de [CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).\n",
    "- **C√≥mo aplicar RL a diferentes disciplinas**, como negocios, videojuegos, rob√≥tica y m√°s.\n",
    "\n",
    "**¬øListo para empezar a explorar el fascinante mundo del aprendizaje por refuerzo?**\n",
    "\n",
    "**¬°Empecemos!üöÄ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
