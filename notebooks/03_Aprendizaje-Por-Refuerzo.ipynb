{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/03_Aprendizaje-Por-Refuerzo.ipynb) üëàüèª‚Äã **Pulsar para abrir en Colab‚Äã**\n",
    "\n",
    "# ¬øC√≥mo usar estos notebooks?\n",
    "\n",
    "Si este es el primer notebook que abres en este repositorio, te recomiendo que antes leas el [Manual de uso de estos notebooks](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/docs/manual-notebooks.md) que he creado para que te familiarices con el proyecto y las distintas rutas que puedes seguir, luego puedes volver aqu√≠ y continuar.\n",
    "\n",
    "En este notebook, vamos a profundizar en el aprendizaje por refuerzo.\n",
    "\n",
    "Por otra parte, si a√∫n no has revisado el notebook \"[00_Empieza-aqu√≠.ipynb](https://github.com/ManuelEspejo/Machine-Learning-Bases/blob/main/notebooks/00_Empieza-aqu√≠.ipynb)\", te sugiero que le eches un vistazo primero para conocer los conceptos b√°sicos. Pero si ya tienes una idea clara de qu√© es el aprendizaje no supervisado y quieres verlo en acci√≥n, ¬°est√°s en el lugar correcto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Aprendizaje por Refuerzo\n",
    "\n",
    "El Aprendizaje por Refuerzo (Reinforcement Learning, RL) es un tipo de Machine Learning donde el modelo (llamado **agente**) aprende a trav√©s de la experiencia en un entorno, intentando maximizar una recompensa acumulada a lo largo del tiempo.\n",
    "\n",
    "## ¬øPor qu√© es importante?\n",
    "\n",
    "El RL es fundamental en la inteligencia artificial moderna porque nos permite resolver problemas en los que las decisiones se deben tomar secuencialmente y las consecuencias de una acci√≥n afectan el futuro. Algunos de los avances m√°s emocionantes en IA han sido gracias al aprendizaje por refuerzo:\n",
    "\n",
    "- **Inteligencias artificiales campeonas en juegos:** Desde el m√≠tico [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) que venci√≥ a los mejores jugadores de Go, hasta bots que dominan videojuegos como [Dota 2](https://arxiv.org/abs/1912.06680) o [StarCraft](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/).\n",
    "- **Rob√≥tica avanzada:** Robots que aprenden a caminar, volar o ensamblar piezas en f√°bricas sin un manual de instrucciones.\n",
    "- **Toma de decisiones aut√≥noma:** Algoritmos que optimizan inversiones, rutas de transporte o sistemas energ√©ticos en tiempo real.\n",
    "- **Ciencia y descubrimiento:** Sistemas que dise√±an medicamentos, exploran galaxias o incluso controlan experimentos cient√≠ficos.\n",
    "\n",
    "En esencia, el RL se utiliza cuando necesitamos que una m√°quina aprende a actuar en un entorno complejo y din√°mico, en el que no hay una soluci√≥n clara de antemano.\n",
    "\n",
    "## ¬øQu√© vamos a ver?\n",
    "\n",
    "En este notebook, vamos a explorar el aprendizaje por refuerzo desde sus fundamentos hasta su implementaci√≥n pr√°ctica. El objetivo es que puedas comprender no solo c√≥mo funciona, sino tambi√©n por qu√© es tan poderoso.\n",
    "\n",
    "Al final de este notebook, entender√°s:\n",
    "\n",
    "- **Los fundamentos del RL**, incluyendo conceptos clave como agente, entorno, recompensa y pol√≠tica.\n",
    "- **C√≥mo funciona un agente RL**, su interacci√≥n con el entorno y c√≥mo aprende para maximizar una recompensa acumulada.\n",
    "- **C√≥mo implementar un modelo RL desde cero**, aplic√°ndolo al cl√°sico problema de [CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).\n",
    "- **C√≥mo aplicar RL a diferentes disciplinas**, como negocios, videojuegos, rob√≥tica y m√°s.\n",
    "\n",
    "**¬øListo para empezar a explorar el fascinante mundo del aprendizaje por refuerzo?**\n",
    "\n",
    "**¬°Empecemos!üöÄ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# √çndice de Contenidos\n",
    "\n",
    "```{table} √çndice\n",
    "1. Fundamentos del Aprendizaje por Refuerzo\n",
    "   1.1. Componentes clave del RL  \n",
    "      1.1.1. Agente, entorno y recompensa  \n",
    "      1.1.2. Pol√≠tica, funci√≥n de valor y funci√≥n Q  \n",
    "   1.2. Exploraci√≥n vs. explotaci√≥n: el equilibrio perfecto  \n",
    "2. Tabular Q-Learning: Un primer paso pr√°ctico\n",
    "   2.1. Descripci√≥n del problema: un agente en una cuadr√≠cula  \n",
    "   2.2. Configuraci√≥n del entorno  \n",
    "   2.3. Implementaci√≥n del algoritmo Tabular Q-Learning  \n",
    "      2.3.1. Inicializaci√≥n de la tabla Q  \n",
    "      2.3.2. Actualizaci√≥n de valores Q   \n",
    "   2.4. Visualizaci√≥n del aprendizaje del agente\n",
    "3. OpenAI Gym y el problema de CartPole\n",
    "   3.1. Introducci√≥n a OpenAI Gym  \n",
    "   3.2. Configuraci√≥n del entorno CartPole  \n",
    "   3.3. Implementaci√≥n de Q-Learning en CartPole  \n",
    "      3.3.1. Entrenamiento del agente  \n",
    "      3.3.2. Visualizaci√≥n del progreso del agente  \n",
    "   3.4. An√°lisis de resultados y reflexiones  \n",
    "4. Casos de Uso y Reflexi√≥n Final\n",
    "   4.1. Aplicaciones reales del RL  \n",
    "      4.1.1. Negocios y marketing  \n",
    "      4.1.2. Rob√≥tica  \n",
    "      4.1.3. Videojuegos  \n",
    "   4.2. Reflexi√≥n sobre las limitaciones y futuro del RL  \n",
    "   4.3. Conclusi√≥n y pr√≥ximos pasos\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones\n",
    "# Detectar si estamos en Colab\n",
    "in_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if in_colab:\n",
    "    # Descargar el archivo visualizations.py desde el repositorio de GitHub\n",
    "    !mkdir -p /content/utils # Creamos una carpeta utils para que coincida con la estructura del repositorio\n",
    "    !wget -O utils/visualizations.py \"https://raw.githubusercontent.com/ManuelEspejo/Machine-Learning-Bases/main/utils/visualizations.py\"\n",
    "    data_dir = '/content/data' # Ruta de los datos\n",
    "else:\n",
    "    # Agregar el directorio ra√≠z al path de Python (Para ejecutar en local)\n",
    "    notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "    project_dir = os.path.dirname(notebook_dir)\n",
    "    sys.path.append(project_dir)\n",
    "    data_dir = '../data/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.visualizations import *  # noqa: F403"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Componentes clave del RL\n",
    "\n",
    "En este apartado, vamos a desglosar los componentes clave del RL para entender bien c√≥mo podemos aplicarlos a nuestros problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Agente, entorno y recompensa\n",
    "\n",
    "En el coraz√≥n del RL, tenemos tres actores principales:\n",
    "\n",
    "#### üßë‚ÄçüöÄ Agente\n",
    "\n",
    "El agente es el protagonista de nuestra historia. Es la entidad que toma decisiones, explora el mundo y aprende de sus errores. En un videojuego, el agente ser√≠a tu personaje controlado. En un robot, el agente ser√≠a el sistema que decide c√≥mo moverse.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: ¬øQu√© acci√≥n debo tomar ahora?\n",
    "\n",
    "#### üåç Entorno\n",
    "\n",
    "El entorno es el mundo donde el agente vive y act√∫a. Define las reglas del juego y las consecuencias de cada acci√≥n. Es como el tablero de un juego de mesa: puede ser un simple tablero 2D o un mundo complejo como el universo de Minecraft.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: ¬øC√≥mo responde el mundo a mis acciones?\n",
    "\n",
    "#### üèÜ Recompensa\n",
    "\n",
    "La recompensa es el \"premio\" que el agente obtiene despu√©s de tomar una acci√≥n en el entorno. Es lo que lo motiva a actuar de cierta manera. En un videojuego, la recompensa podr√≠a ser un punto extra por recoger una moneda. En un agente de bolsa,la recompensa podr√≠a ser la ganancia neta despu√©s de una transacci√≥n.\n",
    "\n",
    "- **Pregunta clave que se hace el agente**: ¬øFue buena mi acci√≥n?\n",
    "\n",
    "Las recompensas son lo que determinan c√≥mo el agente ajusta su comportamiento con el tiempo. El agente b√°sicamente vive para maximizar sus recompensas acumuladas, es su prop√≥sito vital. Seg√∫n la situaci√≥n, podemos encontrar los siguientes escenarios de recompensa:\n",
    "\n",
    "- **Recompensa positiva**: La acci√≥n tomada fue buena. El agente recibe un premio.\n",
    "  - *Ejemplo*: Un dron recibe +1 por cada segundo que se mantiene volando hacia el objetivo.\n",
    "\n",
    "- **Recompensa negativa**: La acci√≥n tomada fue mala. El agente recibe una penalizaci√≥n.\n",
    "  - *Ejemplo*: Si el dron choca contra una pared, recibe una penalizaci√≥n de -10.\n",
    "\n",
    "- **Recompensa nula**: La acci√≥n tomada no tiene consecuencias inmediatas. No es ideal, pero tampoco perjudica al agente.\n",
    "  - *Ejemplo*: El dron empieza a volar en c√≠rculos sin progresar hacia el objetivo.\n",
    "\n",
    "#### Escenarios controlados: Evitar la destruccion del mundo\n",
    "\n",
    "Como habr√°s imaginado, no siempre podemos permitir que el agente practique en el mundo real. Imagina un agente aprendiendo a volar un avi√≥n a base de prueba y error. ¬øQu√© podr√≠a salir mal? O que un agente practicando cirug√≠as a coraz√≥n abierto con pacientes reales. No, graciasüôÇ‚Äç‚ÜîÔ∏è‚Äã.\n",
    "\n",
    "Para evitar el caos (y salvar el mundo), utilizamos simuladores de mundos controlados. Estos entornos simulan de manera segura la realidad, permitiendo que el agente practique, falle y aprende sin causar da√±os en el mundo real.\n",
    "\n",
    "¬°Ojo!, esto no s√≥lo aplica a los agentes; los humanos tambi√©n necesitamos simuladores o entornos controlados para practicar, especialmente en actividades complejas como la conducci√≥n o la cirug√≠a. La diferencia es que un agente descontrolado puede tener un potencial de destrucci√≥n mucho mayor (y no se detiene a reflexionar sobre sus errores como nosotros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Pol√≠tica, funci√≥n de valor y funci√≥n Q\n",
    "\n",
    "Ahora que tenemos claro qui√©n es qui√©n, pasemos a los conceptos que gu√≠an el aprendizaje del agente:\n",
    "\n",
    "#### üéØ Pol√≠tica\n",
    "\n",
    "La pol√≠tica es como el \"cerebro\" del agente. Define qu√© acci√≥n tomar en cada situaci√≥n. Puede ser algo tan simple como una tabla de consulta o tan complejo como una red neuronal.\n",
    "\n",
    "- **Ejemplo:** Si el dron est√° cerca de una pared, la pol√≠tica podr√≠a ser: \"Girar a la izquierda para evitarla\".\n",
    "\n",
    "#### üíé Funci√≥n de valor\n",
    "\n",
    "La funci√≥n de valor le dice al agente qu√© tan bueno es estar en un estado espec√≠fico. Es como si el agente tuviera un mapa que indica qu√© lugares son seguros y cu√°les no.\n",
    "\n",
    "- **Ejemplo:** En el caso del dron, un estado cerca de una pared podr√≠a tener un valor bajo (peligroso), mientras que un estado en el centro de la habitaci√≥n tiene un valor alto (seguro).\n",
    "\n",
    "#### üî¢ Funci√≥n Q\n",
    "\n",
    "La funci√≥n Q es un nivel m√°s avanzado: no solo eval√∫a los estados, sino las acciones dentro de esos estados. Es decir, ayuda al agente a decidir cu√°l acci√≥n espec√≠fica maximizar√° la recompensa.\n",
    "\n",
    "- **Ejemplo:** Si el dron est√° en una esquina, la funci√≥n Q le dir√≠a: \"Girando a la derecha tendr√°s una mejor recompensa que avanzando hacia adelante\".\n",
    "\n",
    "\n",
    "#### Analog√≠a: En busca del tesoro\n",
    "\n",
    "Piensa en un explorador en un bosque. La pol√≠tica es su instinto para decidir si gira a la izquierda o a la derecha. La funci√≥n de valor es el mapa que usa para saber qu√© tan lejos est√° del tesoro. La funci√≥n Q combina ambos: \"Si tomo este camino, ¬øqu√© tan r√°pido llegar√© al tesoro?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Exploraci√≥n vs. explotaci√≥n: el equilibrio perfecto\n",
    "\n",
    "En el aprendizaje por refuerzo encontramos un desaf√≠o fundamental: **¬øcu√°ndo explorar y cu√°ndo explotar?**\n",
    "\n",
    "Para que un agente aprenda de manera eficiente, necesita encontrar el equilibrio perfecto entre estas dos estrategias:\n",
    "\n",
    "### Exploraci√≥n\n",
    "\n",
    "La exploraci√≥n consiste en probar nuevas acciones para descubrir m√°s sobre el entorno, incluso si no garantizan una recompensa inmediata. Es como ser un aventurero que se adentra en territorios desconocidos, con la esperanza de encontrar algo valioso.\n",
    "\n",
    "- **Ventaja**: Puede descubrir estrategias o recompensas que no eran evidentes antes.\n",
    "- **Desventaja**: Puede tomar decisiones sub√≥ptimas en el corto plazo, lo que reduce las recompensas inmediatas.\n",
    "- **Ejemplo**: Un robot que explora una nueva habitaci√≥n podr√≠a intentar atravesar una puerta que no hab√≠a detectado antes, descubriendo un camino m√°s corto hacia su objetivo.\n",
    "\n",
    "### Explotaci√≥n\n",
    "\n",
    "La explotaci√≥n significa usar el conocimiento actual para maximizar las recompensas, eligiendo las acciones que ya sabe que funcionan. Es como ir a tu restaurante favorito y pedir ese plato que nunca te falla. ¬øPor qu√© arriesgarse, verdad?\n",
    "\n",
    "- **Ventaja**: Asegura recompensas constantes y predecibles.\n",
    "- **Desventaja**: Limita el descubrimiento de estrategias potencialmente mejores.\n",
    "- **Ejemplo**: El robot, en lugar de explorar nuevas puertas, siempre usa un camino conocido para llegar a su destino, aunque podr√≠a no ser el m√°s eficiente.\n",
    "\n",
    "### ¬øC√≥mo encontrar el equilibrio?\n",
    "\n",
    "La clave est√° en balancear ambas estrategias. Un agente que solo explora nunca aprovecha lo que ha aprendido, mientras que uno que solo explota se queda atascado en soluciones sub√≥ptimas. Encontrar este equilibrio es esencial para que el agente no solo aprenda, sino que tambi√©n logre maximizar su rendimiento.\n",
    "\n",
    "Esto no es solo un dilema de los agentes; ¬°nos pasa a los humanos todo el tiempo! ¬øDeber√≠as pedir ese plato que sabes que te encanta o arriesgarte a probar algo nuevo? ¬øIr de vacaciones al mismo lugar de siempre o aventurarte a descubrir un destino desconocido? Tanto para los agentes como para nosotros, el truco est√° en ser curiosos sin dejar de aprovechar lo que ya sabemos que funciona.\n",
    "\n",
    "Y t√∫, ¬øeres m√°s explorador o explotador?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tabular Q-Learning: Un primer paso pr√°ctico\n",
    "\n",
    "El **Q-Learning** es uno de los algoritmos m√°s b√°sicos y poderosos en el aprendizaje por refuerzo. Es un m√©todo basado en tablas que permite a un agente aprender la mejor acci√≥n para tomar en cada estado de un entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Descripci√≥n del problema: un agente en una cuadr√≠cula\n",
    "\n",
    "Para este ejemplo, imaginemos que nuestro agente es un explorador en un laberinto de cuadr√≠cula. Su objetivo es encontrar el camino m√°s r√°pido hacia la salida (o el tesoro escondido) sin chocar con los muros ni caer en trampas. Cada celda del laberinto es un estado, y las acciones posibles son moverse en 4 direcciones: arriba, abajo, izquierda o derecha.\n",
    "\n",
    "Estas son las reglas del juego:\n",
    "\n",
    "- **Entorno**: Una cuadr√≠cula 2D (5x5) con una celda de inicio, una meta y varios obst√°culos.\n",
    "- **Estados**: Cada celda de la cuadr√≠cula representa un estado diferente.\n",
    "- **Acciones**: El agente puede moverse en 4 direcciones: arriba, abajo, izquierda o derecha.\n",
    "- **Recompensas**:\n",
    "  - Llegar a la meta: +10 puntos.\n",
    "  - Chocar contra un muro u obst√°culo: -5 puntos.\n",
    "  - Movimiento normal: -1 punto (penalizaci√≥n m√≠nima para evitar movimientos innecesarios).\n",
    "\n",
    "**Visualizaci√≥n del problema:**\n",
    "\n",
    "```plaintext\n",
    "S: Inicio\n",
    "G: Meta\n",
    "X: Obst√°culo\n",
    "\n",
    "    +---+---+---+---+---+\n",
    "    | S |   |   | X |   |\n",
    "    +---+---+---+---+---+\n",
    "    |   |   | X |   |   |\n",
    "    +---+---+---+---+---+\n",
    "    |   | X |   |   | G |\n",
    "    +---+---+---+---+---+\n",
    "    |   |   |   |   |   |\n",
    "    +---+---+---+---+---+\n",
    "    |   |   |   |   |   |\n",
    "    +---+---+---+---+---+\n",
    "\n",
    "```\n",
    "\n",
    "El objetivo del agente en este caso es aprender, a trav√©s de prueba y error, c√≥mo moverse desde la celda de inicio (S) hasta la celda de meta (G) mientras minimiza las penalizaciones.\n",
    "\n",
    "Ahora veamos esto en la pr√°ctica: Configurar esta cuadr√≠cula en Python, inicializar la tabla Q, y comenzar a implementar el algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Configuraci√≥n del entorno\n",
    "\n",
    "Antes de implementar Q-Learning, necesitamos construir nuestro entorno: la cuadr√≠cula 2D donde el agente aprender√° a navegar. Esto incluye definir los estados, las acciones y las reglas del juego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraci√≥n inicial\n",
    "\n",
    "Vamos a definir el entorno como una cuadr√≠cula 5x5, donde cada celda representa un estado. Utilizaremos Python para estructurar la cuadr√≠cula y asignar las recompensas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entorno de recompensas:\n",
      "[[-1 -1 -1 -5 -1]\n",
      " [-1 -1 -5 -1 -1]\n",
      " [-1 -5 -1 -1 10]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "# Tama√±o del entorno\n",
    "grid_size = 5\n",
    "\n",
    "# Matriz de recompensas\n",
    "rewards = np.full((grid_size, grid_size), -1)\n",
    "\n",
    "# Definimos la meta y los obst√°culos\n",
    "rewards[2, 4] = 10\n",
    "rewards[0, 3] = -5 \n",
    "rewards[1, 2] = -5\n",
    "rewards[2, 1] = -5\n",
    "\n",
    "# Mostrar el entorno inicial\n",
    "print(\"Entorno de recompensas:\")\n",
    "print(rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir las acciones\n",
    "\n",
    "El agente puede moverse en cuatro direcciones: **arriba, abajo, izquierda, derecha**. Vamos a asignar un √≠ndice a cada acci√≥n para que sea m√°s f√°cil codificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acciones posibles\n",
    "actions = {\n",
    "    0: \"arriba\",\n",
    "    1: \"abajo\",\n",
    "    2: \"izquierda\",\n",
    "    3: \"derecha\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitar los movimientos\n",
    "\n",
    "Para evitar que el agente se salga de los l√≠mites de la cuadr√≠cula, definiremos una funci√≥n que valide sus movimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_movement(pos, action):\n",
    "    \"\"\"\n",
    "    Valida el movimiento del agente en el entorno.\n",
    "    \"\"\"\n",
    "    x, y = pos\n",
    "    if action == 0 and x > 0:  # Arriba\n",
    "        return (x - 1, y)\n",
    "    elif action == 1 and x < grid_size - 1:  # Abajo\n",
    "        return (x + 1, y)\n",
    "    elif action == 2 and y > 0:  # Izquierda\n",
    "        return (x, y - 1)\n",
    "    elif action == 3 and y < grid_size - 1:  # Derecha\n",
    "        return (x, y + 1)\n",
    "    else:\n",
    "        return pos  # Si el movimiento no es v√°lido, permanece en el mismo lugar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probar el entorno\n",
    "\n",
    "Simulemos un movimiento inicial para verificar que nuestro entorno funciona correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El agente se movi√≥ de (0, 0) a (0, 1)\n",
      "Movimiento inv√°lido: el agente permanece en (0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Posici√≥n inicial del agente\n",
    "pos_inicial = (0, 0)\n",
    "\n",
    "# Acci√≥n de prueba: moverse hacia la derecha\n",
    "nueva_pos = validate_movement(pos_inicial, 3)\n",
    "print(f\"El agente se movi√≥ de {pos_inicial} a {nueva_pos}\")\n",
    "\n",
    "# Acci√≥n inv√°lida: intentar moverse hacia arriba desde el borde superior\n",
    "nueva_pos = validate_movement((0, 0), 0)\n",
    "print(f\"Movimiento inv√°lido: el agente permanece en {nueva_pos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Implementaci√≥n del algoritmo Tabular Q-Learning\n",
    "\n",
    "El Q-Learning es un algoritmo basado en tablas donde el agente aprende a tomar decisiones optimizadas usando la funci√≥n Q, que estima la calidad de las acciones en cada estado. Puedes imaginarlo como una hoja de c√°lculo donde el agente anota qu√© tan buena es cada acci√≥n posible en cada celda o posici√≥n del entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Inicializaci√≥n de la tabla Q\n",
    "\n",
    "La tabla Q es una matriz donde:\n",
    "\n",
    "- Las filas representan los estados.\n",
    "- Las columnas representan las acciones.\n",
    "- Los valores en la tabla indican qu√© tan buena es una acci√≥n en un estado espec√≠fico.\n",
    "\n",
    "Primero, crearemos una tabla Q llena de ceros y definiremos algunos par√°metros clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla Q inicial:\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Inicializar la tabla Q\n",
    "q_table = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "# Par√°metros del Q-Learning\n",
    "alpha = 0.1  # Tasa de aprendizaje\n",
    "gamma = 0.9  # Factor de descuento\n",
    "epsilon = 1.0  # Probabilidad inicial de exploraci√≥n\n",
    "epsilon_decay = 0.99  # Reducci√≥n de epsilon en cada episodio\n",
    "epsilon_min = 0.1  # Valor m√≠nimo de epsilon\n",
    "\n",
    "# Mostrar la tabla Q inicial\n",
    "print(\"Tabla Q inicial:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Nota**: Aqu√≠ podemos ver como los valores de la tabla Q valen 0 ahora mismo, esto es porque el agente no tiene experiencia. A medida que aprende, se actualizar√°n estos valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Actualizaci√≥n de valores Q\n",
    "\n",
    "Para que nuestro agente mejore sus decisiones con el tiempo, es fundamental que actualice continuamente su tabla Q. Este proceso de aprendizaje se basa en una regla sencilla: cada vez que el agente toma una decisi√≥n, registra la recompensa obtenida. De este modo, construye un historial que le permite identificar las acciones que conducen a los mejores resultados.\n",
    "\n",
    "El agente utiliza una forma matem√°tica para ajustar los valores de su tabla Q. La idea es combinar:\n",
    "\n",
    "1. Lo que ya sabe sobre esa acci√≥n.\n",
    "2. Lo que acaba de aprender tras tomar laa acci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso a paso del Q-Learning\n",
    "\n",
    "1. **Elegir una acci√≥n**: El agente decide si explorar (acci√≥n aleatoria) o explotar (acci√≥n con el mejor valor Q conocido).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon):\n",
    "    \"\"\"\n",
    "    Elegir acci√≥n con epsilon-greedy\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(list(actions.keys()))  # Exploraci√≥n\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Explotaci√≥n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta funci√≥n previa, el par√°metro `epsilon` es un valor entre `0` y `1` que determina la probabilidad de que el agente explore, un valor m√°s alto, implica una mayor exploraci√≥n, un valor bajo, implica una mayor explotaci√≥n.\n",
    "\n",
    "2. **Actualizar la tabla Q**: El agente observa la recompensa y el nuevo estado, y actualiza el valor Q de la acci√≥n tomada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q(state, action, reward, new_state):\n",
    "    \"\"\"\n",
    "    Actualizar la tabla Q usando la f√≥rmula de Q-Learning\n",
    "    \"\"\"\n",
    "    max_q_new = np.max(q_table[new_state])\n",
    "    q_table[state][action] += alpha * (reward + gamma * max_q_new - q_table[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Simulaci√≥n de episodios**: Un episodio representa un intento completo del agente por alcanzar la meta desde el estado inicial. Durante cada episodio, el agente toma decisiones, aprende de sus acciones y actualiza la tabla Q. El proceso se repite hasta que el agente llega a la meta o fracasa al no progresar.\n",
    "\n",
    "**Epsilon decay**:\n",
    "\n",
    "Al final de cada episodio, reducimos el valor de `ùúñ` (epsilon) para que el agente explore menos y aproveche m√°s lo aprendido. Sin embargo, nunca lo dejamos llegar a 0, manteniendo un valor m√≠nimo para que el agente siga explorando nuevas acciones de vez en cuando, asegurando un aprendizaje continuo.\n",
    "\n",
    "A continuaci√≥n, implementamos el bucle principal de entrenamiento del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50/500: epsilon=0.61\n",
      "Episode 100/500: epsilon=0.37\n",
      "Episode 150/500: epsilon=0.22\n",
      "Episode 200/500: epsilon=0.13\n",
      "Episode 250/500: epsilon=0.10\n",
      "Episode 300/500: epsilon=0.10\n",
      "Episode 350/500: epsilon=0.10\n",
      "Episode 400/500: epsilon=0.10\n",
      "Episode 450/500: epsilon=0.10\n",
      "Episode 500/500: epsilon=0.10\n"
     ]
    }
   ],
   "source": [
    "# Simulaci√≥n de episodios\n",
    "episodes = 500\n",
    "for episode in range(episodes):\n",
    "    state = (0, 0)  # Inicio\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        # Elegir acci√≥n\n",
    "        action = choose_action(state, epsilon)\n",
    "\n",
    "        # Realizar acci√≥n y observar resultado\n",
    "        new_state = validate_movement(state, action)\n",
    "        reward = rewards[new_state]\n",
    "\n",
    "        # Actualizar Q\n",
    "        update_q(state, action, reward, new_state)\n",
    "\n",
    "        # Actualizar estado\n",
    "        state = new_state\n",
    "\n",
    "        # Verificar si llegamos a la meta\n",
    "        if state == (2, 4):  # Meta\n",
    "            terminated = True\n",
    "\n",
    "    # Reducir epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Mostrar progreso\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode {episode + 1}/{episodes}: epsilon={epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el entrenamiento, observamos c√≥mo el valor de epsilon se reduce gradualmente:\n",
    "\n",
    "- Durante los primeros 50 episodios, el valor de epsilon es alto (0.61), lo que significa que el agente est√° explorando muchas acciones diferentes para aprender m√°s sobre el entorno.\n",
    "- A medida que progresa, epsilon disminuye gradualmente, hasta estabilizarse en el m√≠nimo de 0.10. En esta etapa, el agente explora poco y prefiere explotar las acciones que sabe que funcionan bien.\n",
    "- Mantener un valor m√≠nimo de epsilon asegura que el agente siga probando acciones nuevas ocasionalmente, lo que evita que quede atrapado en soluciones sub√≥ptimas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Visualizaci√≥n del aprendizaje del agente\n",
    "\n",
    "Una vez ha finalizado el entrenamiento, imprimimos la tabla Q aprendida y simulamos un recorrido para verificar que el agente aprendi√≥ correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla Q final:\n",
      "[[[-1.87934664 -0.434062   -2.31014122 -3.29279632]\n",
      "  [-3.79781106 -3.66923476 -1.83789608 -3.76203789]\n",
      "  [-3.56402082 -3.87056828 -3.53604304 -4.65455133]\n",
      "  [-3.29801954 -0.15877916 -1.67326039  1.41791132]\n",
      "  [-0.199       5.85787992 -2.43758337  0.54122307]]\n",
      "\n",
      " [[-2.16174032  0.62882    -1.12522464 -3.00248451]\n",
      "  [-3.37651714 -4.15074628 -1.26136689 -3.4404593 ]\n",
      "  [-1.87776941  1.11488626 -1.77836278  3.85256163]\n",
      "  [-3.54629924  3.20389803 -2.9106375   7.69964307]\n",
      "  [ 0.87696078  9.94846225  2.15240103  5.15676462]]\n",
      "\n",
      " [[-1.40523144  1.8098     -0.16948127 -1.72202604]\n",
      "  [-2.05700781 -0.48318201 -1.60068751  5.74583022]\n",
      "  [-3.4898121   1.87597024 -3.82795092  7.99273952]\n",
      "  [ 4.86483018  5.55533855  5.27481076 10.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.32063692 -0.44691838  1.17636675  3.122     ]\n",
      "  [-1.22103174 -0.13447567  0.89612997  4.58      ]\n",
      "  [ 5.18698369  1.69405061  2.17657179  6.2       ]\n",
      "  [ 8.          2.40852207  3.43336239  4.71490013]\n",
      "  [ 8.78423345 -0.49951     0.1089804   0.95362378]]\n",
      "\n",
      " [[ 1.43900153 -1.89375341 -1.73442444 -1.69566929]\n",
      "  [-0.42515627 -1.47437361 -1.43646584  2.11025359]\n",
      "  [ 4.4795746  -0.42967889 -1.08366566 -0.8295084 ]\n",
      "  [ 5.39642517 -0.27125753 -0.3369115  -0.57717839]\n",
      "  [ 0.3111641  -0.199      -0.57495656 -0.199     ]]]\n",
      "Camino √≥ptimo aprendido por el agente:\n",
      "[(0, 0), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2), (3, 3), (2, 3), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Mostrar tabla Q final\n",
    "print(\"Tabla Q final:\")\n",
    "print(q_table)\n",
    "\n",
    "# Simulaci√≥n de un recorrido √≥ptimo\n",
    "state = (0, 0)\n",
    "path = [state]\n",
    "while state != (2, 4):  # Mientras no lleguemos a la meta\n",
    "    action = np.argmax(q_table[state])\n",
    "    state = validate_movement(state, action)\n",
    "    path.append(state)\n",
    "\n",
    "print(\"Camino √≥ptimo aprendido por el agente:\")\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a los resultados, podemos ver el camino √≥ptimo que el agente ha aprendido para llegar a la meta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizar el camino\n",
    "visualize_q_learning_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro agente sabe c√≥mo navegar la cuadr√≠cula y llegar a la meta mientras evita los obst√°culos. Este ejemplo es simple, pero revela algo poderoso: la capacidad de aprender del entorno y adaptarse con el tiempo.\n",
    "\n",
    "Si nos paramos a pensar, ¬øno es esto similar a nuestras propias vidas? Estamos constantemente explorando caminos, cometiendo errores, encontrando recompensas y aprendiendo. Somos como agentes en nuestro propio entorno, nuestro mundo. siempre buscando nuestro pr√≥ximo movimiento √≥ptimo.\n",
    "\n",
    "Pero, al igual que nuestro agente en la cuadr√≠cula, hay un l√≠mite en lo que podemos hacer con un entorno tan simple. El mundo es vasto, din√°mico y lleno de complejidades. ¬øC√≥mo ser√≠a si pudi√©ramos entrenar a nuestros agentes para enfrentarse a entornos que simulen ese dinamismo? ¬øQu√© secretos podr√≠an desentra√±ar?\n",
    "\n",
    "Con esta pregunta en mente, nos adentramos en **OpenAI Gym**, donde los agentes no solo aprenden, sino que se enfrentan a desaf√≠os m√°s reales y emocionantes. ¬øListo para aplicar lo que hemos aprendido?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. OpenAI Gym y el problema de Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Introducci√≥n a OpenAI Gym\n",
    "\n",
    "OpenAI Gym es una biblioteca que nos proporciona una colecci√≥n de entornos de simulaci√≥n listos para usar. Desde juegos simples hasta problemas complejos de rob√≥tica, es una herramienta esencial para experimentar con el aprendizaje por refuerzo.\n",
    "\n",
    "Un ejemplo cl√°sico que exploraremos aqu√≠ es CartPole: en este entorno, el agente controla un carrito sobre una pista y su objetivo es mantener un palo en equilibrio el mayor tiempo posible. Si el palo se inclina demasiado o el carrito sale de los l√≠mites de la pista, el episodio termina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Configuraci√≥n del entorno CartPole\n",
    "\n",
    "Primero, configuramos el entorno de OpenAI Gym y exploramos sus propiedades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espacio de observaci√≥n: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Espacio de acciones: Discrete(2)\n",
      "Estado inicial: (array([ 0.01238678, -0.00317558,  0.01547069, -0.01760356], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Crear el entorno CartPole\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Observar el espacio de estados y acciones\n",
    "print(\"Espacio de observaci√≥n:\", env.observation_space)\n",
    "print(\"Espacio de acciones:\", env.action_space)\n",
    "\n",
    "# Reiniciar el entorno para obtener el estado inicial\n",
    "state = env.reset()\n",
    "print(\"Estado inicial:\", state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠ podemos ver tres elementos importantes:\n",
    "\n",
    "- **Espacio de observaci√≥n**: El espacio de observaci√≥n nos dice qu√© informaci√≥n puede \"ver\" el agente:\n",
    "    - Posici√≥n del carrito.\n",
    "    - Velocidad del carrito.\n",
    "    - √Ångulo del palo.\n",
    "    - Velocidad angular del palo.\n",
    "- **Espacio de acciones**: En CartPole, el agente puede tomar dos acciones:\n",
    "    - Mover el carrito a la izquierda (acci√≥n 0).\n",
    "    - Mover el carrito a la derecha (acci√≥n 1).\n",
    "- **Estado inicial**: Cuando reiniciamos el entorno, obtenemos un estado inicial que describe la posici√≥n y velocidad del carrito y el palo.\n",
    "\n",
    "Estos datos son el \"mundo\" que el agente puede percibir. Para tomar decisiones, necesita interpretar esta informaci√≥n y aprender c√≥mo afecta sus acciones al equilibrio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizaci√≥n del entorno\n",
    "\n",
    "Vamos a visualizar CartPole en acci√≥n para entender mejor c√≥mo se comporta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ö†Ô∏è‚Äã **Nota importante**: Si encuentras un error relacionado con `np.bool8` al ejecutar este c√≥digo, es posible que necesites modificar el archivo `passive_env_checker.py` en la instalaci√≥n de Gym.\n",
    "\n",
    "El error suele ser:\n",
    "```python\n",
    "AttributeError: module 'numpy' has no attribute 'bool8'\n",
    "```\n",
    "\n",
    "**Soluci√≥n**:\n",
    "1. Localiza el archivo: `gym/utils/passive_env_checker.py` en tu sistema.\n",
    "2. Reemplaza todas las instancias de `np.bool8` por `np.bool_`.\n",
    "3. Guarda el archivo, y ejecuta el c√≥digo nuevamente.\n",
    "4. Si sigues teniendo problemas, prueba a reiniciar el entorno de ejecuci√≥n y ejecuta nuevamente todo el c√≥digo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el entorno CartPole\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Simulaci√≥n con pausa entre frames\n",
    "for _ in range(200):\n",
    "    env.render()  # Mostrar el entorno\n",
    "    action = env.action_space.sample()  # Elegir acci√≥n aleatoria\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    time.sleep(0.005)  # A√±adir una peque√±a pausa para ver mejor la simulaci√≥n\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()  # Reiniciar si el episodio termina\n",
    "        time.sleep(0.5)  # Pausa entre episodios\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes observar, el carrito intenta mantener el palo en equilibrio el mayor tiempo posible, movi√©ndose de un lado a otro para evitar que se caiga o que salga de los l√≠mites de la pista. Sin embargo, cuando el palo supera un cierto √°ngulo de inclinaci√≥n o el carrito se desplaza demasiado lejos, el episodio termina y el entorno se reinicia autom√°ticamente.\n",
    "\n",
    "En particular, el entorno se reinicia cuando:\n",
    "- El palo se inclina m√°s de 12 grados.\n",
    "- El carrito se desplaza m√°s de 2.4 unidades de distancia del centro.\n",
    "\n",
    "Estos criterios est√°n predefinidos en OpenAI Gym para simplificar el problema y permitir que el agente pueda enfocarse en estrategias claras para maximizar su recompensa dentro de un marco limitado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Implementaci√≥n de Q-Learning para CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TallerML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
